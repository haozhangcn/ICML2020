# ICML2020
ICML2020 papers with abstract

### 1.Reverse-engineering deep ReLU networks [:chains:](https://proceedings.icml.cc/book/3241.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/1-Paper.pdf)
  David Rolnick, Konrad Kording[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/1-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/1-Supplemental.zip)
> It has been widely assumed that a neural network cannot be recovered from its outputs, as the  network depends on its parameters in a highly nonlinear way. Here, we prove that in fact it is often possible to identify the architecture, weights, and biases of an unknown deep ReLU network by observing only its output. Every ReLU network defines a piecewise linear function, where the boundaries between linear regions correspond to inputs for which some neuron in the network switches between inactive and active ReLU states. By dissecting the set of region boundaries into components associated with particular neurons, we show both theoretically and empirically that it is possible to recover the weights of neurons and their arrangement within the network, up to isomorphism.
### 2.My Fair Bandit: Distributed Learning of Max-Min Fairness with Multi-player Bandits [:chains:](https://proceedings.icml.cc/book/3242.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/11-Paper.pdf)
  Ilai Bistritz, Tavor Baharav, Amir Leshem, Nicholas Bambos[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/11-Metadata.json)
> Consider N cooperative but non-communicating players where each plays one out of M arms for T turns. Players have different utilities for each arm, representable as an NxM matrix. However, these utilities are unknown to the players. In each turn players receive noisy observations of their utility for their selected arm. However, if any other players selected the same arm that turn, they will all receive zero utility due to the conflict. No other communication or coordination between the players is possible. Our goal is to design a distributed algorithm that learns the matching between players and arms that achieves max-min fairness while minimizing the regret. We present an algorithm and prove that it is regret optimal up to a log(log T) factor. This is the first max-min fairness multi-player bandit algorithm with (near) order optimal regret. 
### 3.Scalable Differentiable Physics for Learning and Control [:chains:](https://proceedings.icml.cc/book/3243.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/15-Paper.pdf)
  Yi-Ling Qiao, Junbang Liang, Vladlen Koltun, Ming Lin[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/15-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/15-Supplemental.pdf)
> Differentiable physics is a powerful approach to learning and control problems that involve physical objects and environments. While notable progress has been made, the capabilities of differentiable physics solvers remain limited. We develop a scalable framework for differentiable physics that can support a large number of objects and their interactions. To accommodate objects with arbitrary geometry and topology, we adopt meshes as our representation and leverage the sparsity of contacts for scalable differentiable collision handling. Collisions are resolved in localized regions to minimize the number of optimization variables even when the number of simulated objects is high. We further accelerate implicit differentiation of optimization with nonlinear constraints. Experiments demonstrate that the presented framework requires up to two orders of magnitude less memory and computation in comparison to recent particle-based methods. We further validate the approach on inverse problems and control scenarios, where it outperforms derivative-free and model-free baselines by at least an order of magnitude.
### 4.Generalization to New Actions in Reinforcement Learning [:chains:](https://proceedings.icml.cc/book/3244.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/29-Paper.pdf)
  Ayush Jain, Andrew Szot, Joseph Lim[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/29-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/29-Supplemental.pdf)
> A fundamental trait of intelligence is the ability to achieve goals in the face of novel circumstances. However, standard reinforcement learning typically assumes a fixed set of actions to choose from. Completing tasks with a new action space then requires time-consuming retraining. The ability to seamlessly utilize novel actions is crucial for adaptable agents. We take a step in this direction by introducing the problem of learning to generalize decision-making to unseen actions, based on action information acquired separately from the task. To approach this problem, we propose a two-stage framework where the agent first infers action representations from acquired action observations and then learns to use these in reinforcement learning with added generalization objectives. We demonstrate that our framework enables zero-shot generalization to new actions in sequential decision-making tasks, such as selecting unseen tools to solve physical reasoning puzzles and stacking towers with novel 3D shapes.
### 5.Randomized Block-Diagonal Preconditioning for Parallel Learning [:chains:](https://proceedings.icml.cc/book/3245.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/53-Paper.pdf)
  Celestine Mendler-DÃ¼nner, Aurelien Lucchi[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/53-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/53-Supplemental.pdf)
> We study preconditioned gradient-based optimization methods where the preconditioning matrix has block-diagonal form. Such a structural constraint comes with the advantage that the update computation can be parallelized across multiple independent tasks. Our main contribution is to demonstrate that the convergence of these methods can significantly be improved by a randomization technique which corresponds to repartitioning coordinates across tasks during the optimization procedure. We provide a theoretical analysis that accurately characterizes the expected convergence gains of repartitioning and validate our findings empirically on various traditional machine learning tasks. From an implementation perspective, block-separable models are well suited for parallelization and, when shared memory is available, randomization can be implemented on top of existing methods very efficiently to improve convergence.
### 6.Stochastic Flows and Geometric Optimization on the Orthogonal Group [:chains:](https://proceedings.icml.cc/book/3246.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/57-Paper.pdf)
  Krzysztof Choromanski, David Cheikhi, Jared Davis, Valerii Likhosherstov, Achille Nazaret, Achraf Bahamou, Xingyou Song, Mrugank Akarte, Jack Parker-Holder, Jacob Bergquist, YUAN GAO, Aldo Pacchiano, Tamas Sarlos, Adrian Weller, Vikas Sindhwani[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/57-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/57-Supplemental.pdf)
> We present a new class of stochastic, geometrically-driven optimization algorithms on the orthogonal group O(d) and naturally reductive homogeneous manifolds obtained from the action of the rotation group SO(d). We theoretically and experimentally demonstrate that our methods can be applied in various fields of machine learning including deep, convolutional and recurrent neural networks, reinforcement learning, normalizing flows and metric learning. We show an intriguing connection between efficient stochastic optimization on the orthogonal group and graph theory (e.g. matching problem, partition functions over graphs, graph-coloring). We leverage the theory of Lie groups and provide theoretical results for the designed class of algorithms. We demonstrate broad applicability of our methods by showing strong performance on the seemingly unrelated tasks of learning world models to obtain stable policies for the most difficult Humanoid agent from OpenAI Gym and improving convolutional neural networks.
### 7.PackIt: A Virtual Environment for Geometric Planning [:chains:](https://proceedings.icml.cc/book/3247.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/62-Paper.pdf)
  Ankit Goyal, Jia Deng[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/62-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/62-Supplemental.pdf)
> The ability to jointly understand the geometry of objects and plan actions for manipulating them is crucial for intelligent agents. We refer to this ability as geometric planning. Recently, many interactive environments have been proposed to evaluate intelligent agents on various skills, however, none of them cater to the needs of geometric planning. We present PackIt, a virtual environment to evaluate and potentially learn the ability to do geometric planning. In this environment, an agent needs to take a sequence of actions to pack a set of objects into a box with limited space. We also construct a set of challenging packing tasks using an evolutionary algorithm. Further, we study various baselines for the task that include model-free learning-based and heuristic-based methods, as well as search-based optimization methods that assume access to the model of the environment.
### 8.Soft Threshold Weight Reparameterization for Learnable Sparsity [:chains:](https://proceedings.icml.cc/book/3248.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/67-Paper.pdf)
  Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, Ali Farhadi[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/67-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/67-Supplemental.pdf)
> Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and MobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics. Code, pretrained models and sparsity budgets are at https://github.com/RAIVNLab/STR.
### 9.Stochastic Latent Residual Video Prediction [:chains:](https://proceedings.icml.cc/book/3249.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/78-Paper.pdf)
  Jean-Yves Franceschi, Edouard Delasalles, Mickael Chen, Sylvain Lamprier, Patrick Gallinari[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/78-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/78-Supplemental.zip)
> Designing video prediction models that account for the inherent uncertainty of the future is challenging. Most works in the literature are based on stochastic image-autoregressive recurrent networks, which raises several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and temporal dynamics. However, no such model for stochastic video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model whose dynamics are governed in a latent space by a residual update rule. This first-order scheme is motivated by discretization schemes of differential equations. It naturally models video dynamics as it allows our simpler, more interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.
### 10.Fractional Underdamped Langevin Dynamics: Retargeting SGD with Momentum under Heavy-Tailed Gradient Noise [:chains:](https://proceedings.icml.cc/book/3250.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/86-Paper.pdf)
  Umut Simsekli, Lingjiong Zhu, Yee Whye Teh, Mert Gurbuzbalaban[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/86-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/86-Supplemental.pdf)
> Stochastic gradient descent with momentum (SGDm) is one of the most popular optimization algorithms in deep learning. While there is a rich theory of SGDm for convex problems, the theory is considerably less developed in the context of deep learning where the  problem is non-convex and the gradient noise might exhibit a heavy-tailed behavior, as empirically observed in recent studies. In this study, we consider a \emph{continuous-time} variant of SGDm, known as the underdamped Langevin dynamics (ULD), and investigate its asymptotic properties under heavy-tailed perturbations. Supported by recent studies from statistical physics, we argue both theoretically and empirically that the heavy-tails of such perturbations can result in a bias even when the step-size is small, in the sense that \emph{the optima of stationary distribution} of the dynamics might not match \emph{the optima of the cost function to be optimized}. As a remedy, we develop a novel framework, which we coin as \emph{fractional} ULD (FULD), and prove that FULD targets the so-called Gibbs distribution, whose optima exactly match the optima of the original cost. We observe that the Euler discretizatin of FULD has noteworthy algorithmic similarities with \emph{natural gradient} methods and \emph{gradient clipping}, bringing a new perspective on understanding their role in deep learning. We support our theory with experiments conducted on a synthetic model and neural networks.
### 11.Context Aware Local Differential Privacy [:chains:](https://proceedings.icml.cc/book/3251.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/111-Paper.pdf)
  Jayadev Acharya, Keith Bonawitz, Peter Kairouz, Daniel  Ramage, Ziteng Sun[Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/111-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/111-Supplemental.pdf)
> Local differential privacy (LDP) is a strong notion of privacy that often leads to a significant drop in utility. The original definition of LDP assumes that all the elements in the data domain are equally sensitive. However, in many real-life applications, some elements are more sensitive than others. We propose a context-aware framework for LDP that allows the privacy level to vary across the data domain, enabling system designers to place privacy constraints where they matter without paying the cost where they do not. For binary data domains, we provide a universally optimal privatization scheme and highlight its connections to Warnerâs randomized response and Mangatâs improved response. Motivated by geo-location and web search applications, for k-ary data domains, we consider two special cases of context-aware LDP: block-structured LDP and high-low LDP. We study minimax discrete distribution estimation under both cases and provide communication-efficient, sample-optimal schemes, and information-theoretic lower bounds. We show, using worst-case analyses and experiments on Gowallaâs 3.6 million check-ins to 43,750 locations, that context-aware LDP achieves a far better accuracy under the same number of samples.


