# ICML2020-4
ICML2020 papers with abstract [:link:](https://proceedings.icml.cc/book/2020)

### 991.[Decision Trees for Decision-Making under the Predict-then-Optimize Framework](https://proceedings.icml.cc/book/4232.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6150-Paper.pdf)
  Adam Elmachtoub, Jason Cheuk Nam Liang, Ryan McNellis [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6150-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6150-Supplemental.pdf)
> <p>We consider the use of decision trees for decision-making problems under the predict-then-optimize framework. That is, we would like to first use a decision tree to predict unknown input parameters of an optimization problem, and then make decisions by solving the optimization problem using the predicted parameters. A natural loss function in this framework is to measure the suboptimality of the decisions induced by the predicted input parameters, as opposed to measuring loss using input parameter prediction error. This natural loss function is known in the literature as the Smart Predict-then-Optimize (SPO) loss, and we propose a tractable methodology called SPO Trees (SPOTs) for training decision trees under this loss. SPOTs benefit from the interpretability of decision trees, providing an interpretable segmentation of contextual features into groups with distinct optimal solutions to the optimization problem of interest. We conduct several numerical experiments on synthetic and real data including the prediction of travel times for shortest path problems and predicting click probabilities for news article recommendation. We demonstrate on these datasets that SPOTs simultaneously provide higher quality decisions and significantly lower model complexity than other machine learning approaches (e.g., CART) trained to minimize prediction error.</p> 
### 992.[Representation Learning via Adversarially-Contrastive Optimal Transport](https://proceedings.icml.cc/book/4233.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6155-Paper.pdf)
  Anoop Cherian, Shuchin Aeron [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6155-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6155-Supplemental.pdf)
> <p>In this paper, we study the problem of learning compact (low-dimensional) representations for sequential data that captures its implicit spatio-temporal cues. To maximize extraction of such informative cues from the data, we set the problem within the context of contrastive representation learning and to that end propose a novel objective via optimal transport. Specifically, our formulation seeks a low-dimensional subspace representation of the data that jointly (i) maximizes the distance of the data (embedded in this subspace) from an adversarial data distribution under the optimal transport, a.k.a. the Wasserstein distance, (ii) captures the temporal order, and (iii) minimizes the data distortion. To generate the adversarial distribution, we propose to use a Generative Adversarial Network (GAN) with novel regularizers. Our full objective can be cast as a subspace learning problem on the Grassmann manifold, and can be solved efficiently via Riemannian optimization. To empirically study our formulation, we provide elaborate experiments on the task of human action recognition in video sequences. Our results demonstrate state-of-the-art performance against challenging baselines.</p> 
### 993.[Neuro-Symbolic Visual Reasoning: Disentangling &quot;Visual&quot; from &quot;Reasoning&quot;](https://proceedings.icml.cc/book/4234.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6156-Paper.pdf)
  Saeed Amizadeh, Hamid Palangi, Oleksandr Polozov, Yichen Huang, Kazuhito Koishida [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6156-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6156-Supplemental.pdf)
> <p>Visual reasoning tasks such as visual question answering (VQA) require an interplay of visual perception with reasoning about the question semantics grounded in perception. Challenges like VCR (Zellers et al., 2019) and GQA (Hudson&amp; Manning, 2019) facilitate scientific progress from perception models to visual reasoning. However, recent advances on GQA are still primarily driven by perception improvements (e.g.  scene graph generation) rather than reasoning. Neuro-symbolic models such as MAC (Hudson&amp; Manning, 2018) bring the benefits of compositional reasoning to VQA, but they are still entangled with visual representation learning, and thus neural reasoning is hard to improve and assess on its own.</p>  <p>To address this, we propose (1) a framework to isolate and evaluate the reasoning aspect of VQA separately from its perception, and (2) a novel top-down calibration technique that allows the model to answer reasoning questions even with imperfect perception. To this end, we introduce a differentiable first-order logic formalism for VQA that explicitly decouples question answering from visual perception. On the challenging GQA dataset, this approach is competitive with non-symbolic neural models while also interpretable by construction, composable with arbitrary pre-trained visual representation learning, and requires much fewer parameters.</p> 
### 994.[Two Simple Ways to Learn Individual Fairness Metric from Data](https://proceedings.icml.cc/book/4235.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6161-Paper.pdf)
  Debarghya Mukherjee, Mikhail Yurochkin, Moulinath Banerjee, Yuekai Sun [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6161-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6161-Supplemental.pdf)
> <p>Individual fairness was proposed to address some of the shortcomings of group fairness. Despite its benefits, it requires a task specific fairness metric that encodes our intuition of what is fair and what is unfair for the ML task at hand. Ambiguity in this metric is the main barrier to wider adoption of individual fairness. In this paper, we present two simple algorithms that learn effective fair metrics from a variety of datasets. We verify empirically that fair training with these metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches. </p> 
### 995.[A Simple Framework for Contrastive Learning of Visual Representations](https://proceedings.icml.cc/book/4236.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6165-Paper.pdf)
  Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6165-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6165-Supplemental.pdf)
> <p>This paper presents a simple framework for contrastive representation learning. The framework, SimCLR, simplifies recently proposed approaches and requires neither specific architectural modifications nor a memory bank. In order to understand what enables the contrastive prediction task to learn useful representations, we systematically study the major components in the framework. We empirically show that 1) composition of data augmentations plays a critical role in defining the predictive tasks that enable effective representation learning, 2) introducing a learned nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the representation, and 3) contrastive learning benefits from a larger batch size and more training steps compared to the supervised counterpart. By combining our findings, we improve considerably over previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on the representation of our best model achieves 76.5% top-1 accuracy, a 7% relative improvement over previous state-of-the-art. When fine-tuned on 1% of labels, our model achieves 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.</p> 
### 996.[The Implicit and Explicit Regularization Effects of Dropout](https://proceedings.icml.cc/book/4237.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6166-Paper.pdf)
  Colin Wei, Sham Kakade, Tengyu Ma [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6166-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6166-Supplemental.zip)
> <p>Dropout is a widely-used regularization technique, often required to obtain state-of-the-art for a number of architectures. This work observes that dropout introduces two distinct but entangled regularization effects: an explicit effect which occurs since dropout modifies the expected training objective, and an implicit effect from stochasticity in the dropout gradients. We disentangle these two effects, deriving analytic simplifications which characterize each effect in terms of the derivatives of the model and loss. Our simplified regularizers accurately capture the important aspects of dropout: we demonstrate that they can faithfully replace dropout in practice.</p> 
### 997.[Variable-Bitrate Neural Compression via Bayesian Arithmetic Coding](https://proceedings.icml.cc/book/4238.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6168-Paper.pdf)
  Yibo Yang, Robert Bamler, Stephan Mandt [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6168-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6168-Supplemental.pdf)
> <p>Deep Bayesian latent variable models have enabled new approaches to both model and data compression. Here, we propose a new algorithm for compressing latent representations in deep probabilistic models, such as variational autoencoders, in post-processing. The approach thus separates model design and training from the compression task. Our algorithm generalizes arithmetic coding to the continuous domain, using adaptive discretization accuracy that exploits estimates of posterior uncertainty. A consequence of the "plug and play" nature of our approach is that various rate-distortion trade-offs can be achieved with a single trained model, eliminating the need to train and store multiple models for different bit rates. Our experimental results demonstrate the importance of taking into account posterior uncertainties, and show that image compression with the proposed algorithm outperforms JPEG over a wide range of bit rates using only a single machine learning model. Further experiments on Bayesian neural word embeddings demonstrate the versatility of the proposed method.</p> 
### 998.[Orthogonalized SGD and Nested Architectures for Anytime Neural Networks](https://proceedings.icml.cc/book/4239.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6169-Paper.pdf)
  Chengcheng Wan, Henry (Hank) Hoffmann, Shan Lu, Michael Maire [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6169-Metadata.json)
> <p>We propose a novel variant of SGD customized for training network architectures that support anytime behavior: such networks produce a series of increasingly accurate outputs over time.  Efficient architectural designs for these networks focus on re-using internal state; subnetworks must produce representations relevant for both immediate prediction as well as refinement by subsequent network stages.  We consider traditional branched networks as well as a new class of recursively nested networks. Our new optimizer, Orthogonalized SGD, dynamically re-balances task-specific gradients when training a multitask network.  In the context of anytime architectures, this optimizer projects gradients from later outputs onto a parameter subspace that does not interfere with those from earlier outputs.  Experiments demonstrate that training with Orthogonalized SGD significantly improves generalization accuracy of anytime networks.</p> 
### 999.[Evaluating Machine Accuracy on ImageNet](https://proceedings.icml.cc/book/4240.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6173-Paper.pdf)
  Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex Fang, Benjamin Recht, Ludwig Schmidt [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6173-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6173-Supplemental.pdf)
> <p>We perform an in-depth evaluation of human accuracy on the ImageNet dataset. First, three expert labelers re-annotated 30,000 images from the original ImageNet validation set and the ImageNetV2 replication experiment with multi-label annotations to enable a semantically coherent accuracy measurement. Then we evaluated five trained humans on both datasets. The median of the five labelers outperforms the best publicly released ImageNet model by 1.5% on the original validation set and by 6.2% on ImageNetV2. Moreover, the human labelers see a substantially smaller drop in accuracy between the two datasets compared to the best available model (less than 1% vs 5.4%). Our results put claims of superhuman performance on ImageNet in context and show that robustly classifying ImageNet at human-level performance is still an open problem.</p> 
### 1000.[Learning to Navigate in Synthetically Accessible Chemical Space Using Reinforcement Learning](https://proceedings.icml.cc/book/4241.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6186-Paper.pdf)
  Sai Krishna Gottipati, Boris Sattarov, Sufeng Niu, Haoran Wei, Yashaswi Pathak, Shengchao Liu, Simon Blackburn, Karam Thomas, Connor Coley, Jian Tang, Sarath Chandar, Yoshua Bengio [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6186-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6186-Supplemental.pdf)
> <p>Over the last decade, there has been significant progress in the field of machine learning-based de novo drug discovery, particularly in generative modeling of chemical structures. However, current generative approaches exhibit a significant challenge: they do not ensure the synthetic accessibility nor provide the synthetic routes of the proposed small molecules which limits their applicability. In this work, we propose a novel reinforcement learning (RL) setup for drug discovery that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo compound design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting initial commercially available molecules to valid chemical reactions at every time step of the iterative virtual synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. Our end-to-end approach achieves state-of-the-art performance when compared against other generative approaches for drug discovery. Moreover, we leverage our approach in a proof-of-concept that mimics the drug discovery process by generating novel HIV drug candidates. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.</p> 
### 1001.[Improved Bounds on Minimax Regret under Logarithmic Loss via Self-Concordance](https://proceedings.icml.cc/book/4242.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6190-Paper.pdf)
  Blair Bilodeau, Dylan Foster, Daniel Roy [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6190-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6190-Supplemental.pdf)
> We study the classical problem of forecasting under logarithmic loss while competing against an arbitrary class of experts. We present a novel approach to bounding the minimax regret that exploits the self-concordance property of logarithmic loss. Our regret bound depends on the metric entropy of the expert class and matches previous best known results for arbitrary expert classes. We improve the dependence on the time horizon for classes with metric entropy under the supremum norm of order $\Omega(\gamma^{-p})$ when $p&gt;1$, which includes, for example, Lipschitz functions of dimension greater than 1. 
### 1002.[Optimization Theory for ReLU Neural Networks Trained with Normalization Layers](https://proceedings.icml.cc/book/4243.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6192-Paper.pdf)
  Yonatan Dukler, Quanquan Gu, Guido Montufar [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6192-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6192-Supplemental.pdf)
> <p>The current paradigm of deep neural networks has been successful in part due to the use of normalization layers. Normalization layers like Batch Normalization, Layer Normalization and Weight Normalization are ubiquitous in practice as they improve the generalization performance and training speed of neural networks significantly. Nonetheless, the vast majority of current deep learning theory and non-convex optimization literature focuses on the un-normalized setting.  We bridge this gap by providing the first global convergence result for 2 layer non-linear neural networks with ReLU activations trained with a normalization layer, namely Weight Normalization. The analysis shows how the introduction of normalization layers changes the optimization landscape and in some settings enables faster convergence as compared with un-normalized neural networks. </p> 
### 1003.[Improving Molecular Design by Stochastic Iterative Target Augmentation](https://proceedings.icml.cc/book/4244.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6209-Paper.pdf)
  Kevin Yang, Wengong Jin, Kyle Swanson, Regina Barzilay, Tommi Jaakkola [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6209-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6209-Supplemental.pdf)
> <p>Generative models in molecular design tend to be richly parameterized, data-hungry neural models, as they must create complex structured objects as outputs. Estimating such models from data may be challenging due to the lack of sufficient training data. In this paper, we propose a surprisingly effective self-training approach for iteratively creating additional molecular targets. We first pre-train the generative model together with a simple property predictor. The property predictor is then used as a likelihood model for filtering candidate structures from the generative model. Additional targets are iteratively produced and used in the course of stochastic EM iterations to maximize the log-likelihood that the candidate structures are accepted. A simple rejection (re-weighting) sampler suffices to draw posterior samples since the generative model is already reasonable after pre-training. We demonstrate significant gains over strong baselines for both unconditional and conditional molecular design. In particular, our approach outperforms the previous state-of-the-art in conditional molecular design by over 10% in absolute gain.</p> 
### 1004.[Don&#x27;t Waste Your Bits! Squeeze Activations and Gradients for Deep Neural Networks via TinyScript](https://proceedings.icml.cc/book/4245.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6210-Paper.pdf)
  Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, Bin Cui [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6210-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6210-Supplemental.pdf)
> <p>Recent years have witnessed intensive research interests on training deep neural networks (DNNs) more efficiently by quantization-based compression methods, which facilitate DNNs training in two ways: (1) activations are quantized to shrink the memory consumption, and (2) gradients are quantized to decrease the communication cost. However, existing methods mostly use a uniform mechanism that quantizes the values evenly. Such a scheme may cause a large quantization variance and slow down the convergence in practice.</p>  <p>In this work, we introduce TinyScript, which applies a non-uniform quantization algorithm to both activations and gradients. TinyScript models the original values by a family of Weibull distributions and searches for ''quantization knobs'' that minimize quantization variance. We also discuss the convergence of the non-uniform quantization algorithm on DNNs with varying depths, shedding light on the number of bits required for convergence. Experiments show that TinyScript always obtains lower quantization variance, and achieves comparable model qualities against full precision training using 1-2 bits less than the uniform-based counterpart.</p> 
### 1005.[Robust One-Bit Recovery via ReLU Generative Networks: Near-Optimal Statistical Rate and Global Landscape Analysis](https://proceedings.icml.cc/book/4246.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6234-Paper.pdf)
  Shuang Qiu, Xiaohan Wei, Zhuoran Yang [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6234-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6234-Supplemental.pdf)
> We study the robust one-bit compressed sensing problem whose goal is to design an algorithm that faithfully recovers any sparse target vector $\theta_0\in\mathbb{R}^d$  \textit{uniformly} via $m$ quantized noisy measurements. Specifically, we consider a new framework for this problem where the sparsity is implicitly enforced via mapping a low dimensional representation $x_0 \in \RR^k$  through a known $n$-layer ReLU generative network $G:\mathbb{R}^k\rightarrow\mathbb{R}^d$ such that $\theta_0 = G(x_0)$.  Such a framework poses low-dimensional priors on $\theta_0$ without a known sparsity basis. We propose to recover the target $G(x_0)$ solving an unconstrained empirical risk minimization (ERM). Under a weak \textit{sub-exponential measurement assumption}, we establish a joint statistical and computational analysis. In particular, we prove that the ERM estimator in this new framework achieves a statistical rate of $m=\tilde{\mathcal{O}}(kn \log d /\varepsilon^2)$ recovering any $G(x_0)$ uniformly up to an error $\varepsilon$. When the network is shallow (i.e., $n$ is small), we show this rate matches the information-theoretic lower bound up to logarithm factors on $\varepsilon^{-1}$. From the lens of computation, we prove that under proper conditions on the ReLU weights, our proposed empirical risk, despite non-convexity, has no stationary point outside of small neighborhoods around the true representation $x_0$ and its negative multiple. Furthermore, we show that the global minimizer of the empirical risk stays within the neighborhood around $x_0$ rather than its negative multiple under further assumptions on ReLU weights.
### 1006.[Multi-objective Bayesian Optimization using Pareto-frontier Entropy](https://proceedings.icml.cc/book/4247.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6243-Paper.pdf)
  Shinya Suzuki, Shion Takeno, Tomoyuki Tamura, Kazuki Shitara, Masayuki Karasuyama [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6243-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6243-Supplemental.zip)
> <p>This paper studies an entropy-based multi-objective Bayesian optimization (MBO). The entropy search is successful approach to Bayesian optimization. However, for MBO, existing entropy-based methods ignore trade-off among objectives or introduce unreliable approximations. We propose a novel entropy-based MBO called Pareto-frontier entropy search (PFES) by considering the entropy of Pareto-frontier, which is an essential notion of the optimality of the multi-objective problem. Our entropy can incorporate the trade-off relation of the optimal values, and further, we derive an analytical formula without introducing additional approximations or simplifications to the standard entropy search setting. We also show that our entropy computation is practically feasible by using a recursive decomposition technique which has been known in studies of the Pareto hyper-volume computation. Besides the usual MBO setting, in which all the objectives are simultaneously observed, we also consider the ``decoupled'' setting, in which the objective functions can be observed separately. PFES can easily adapt to the decoupled setting by considering the entropy of the marginal density for each output dimension. This approach incorporates dependency among objectives conditioned on Pareto-frontier, which is ignored by the existing method. Our numerical experiments show effectiveness of PFES through several benchmark datasets.</p> 
### 1007.[Closing the convergence gap of SGD without replacement](https://proceedings.icml.cc/book/4248.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6245-Paper.pdf)
  Shashank Rajput, Anant Gupta, Dimitris Papailiopoulos [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6245-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6245-Supplemental.pdf)
> Stochastic gradient descent without replacement sampling is widely used in practice for model training. However, the vast majority of SGD analyses assumes data sampled with replacement, and when the function minimized is strongly convex, an $\mathcal{O}\left(\frac{1}{T}\right)$ rate can be established when SGD is run for $T$ iterations. A recent line of breakthrough work on SGD without replacement (SGDo) established an $\mathcal{O}\left(\frac{n}{T^2}\right)$ convergence rate when the function minimized is strongly convex and is a sum of $n$ smooth functions, and an $\mathcal{O}\left(\frac{1}{T^2}+\frac{n^3}{T^3}\right)$ rate for sums of quadratics. On the other hand, the tightest known lower bound postulates an $\Omega\left(\frac{1}{T^2}+\frac{n^2}{T^3}\right)$ rate, leaving open the possibility of better SGDo convergence rates in the general case. In this paper, we close this gap and show that SGD without replacement achieves a rate of $\mathcal{O}\left(\frac{1}{T^2}+\frac{n^2}{T^3}\right)$ when the sum of the functions is a quadratic, and offer a new lower bound of $\Omega\left(\frac{n}{T^2}\right)$ for strongly convex functions that are sums of smooth functions.
### 1008.[Black-Box Methods for Restoring Monotonicity](https://proceedings.icml.cc/book/4249.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6248-Paper.pdf)
  Evangelia Gergatsouli, Brendan Lucier, Christos Tzamos [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6248-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6248-Supplemental.pdf)
> In many practical applications, heuristic or approximation algorithms are used to efficiently solve the task at hand. However their solutions frequently do not satisfy natural monotonicity properties expected to hold in the optimum. In this work we develop algorithms that are able to restore monotonicity in the parameters of interest.  Specifically, given oracle access to a possibly non monotone function,  we provide an algorithm that restores monotonicity while degrading the expected value of the function by at most $\epsilon$. The number of queries required is at most logarithmic in $1/\epsilon$ and exponential in the number of parameters. We also give a lower bound showing that this exponential dependence is necessary.  Finally, we obtain improved query complexity bounds for restoring the weaker property of $k$-marginal monotonicity. Under this property, every $k$-dimensional projection of the function is required to be monotone. The query complexity we obtain only scales exponentially with $k$ and is polynomial in the number of parameters. 
### 1009.[Flexible and Efficient Long-Range Planning Through Curious Exploration](https://proceedings.icml.cc/book/4250.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6249-Paper.pdf)
  Aidan Curtis, Minjian Xin, Dilip Arumugam, Kevin Feigelis, Daniel Yamins [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6249-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6249-Supplemental.pdf)
> <p>Identifying algorithms that flexibly and efficiently discover temporally-extended multi-phase plans is an essential step for the advancement of robotics and model-based reinforcement learning. The core problem of long-range planning is finding an efficient way to search through the tree of possible action sequences. Existing non-learned planning solutions from the Task and Motion Planning (TAMP) literature rely on the existence of logical descriptions for the effects and preconditions for actions. This constraint allows TAMP methods to efficiently reduce the tree search problem but limits their ability to generalize to unseen and complex physical environments. In contrast, deep reinforcement learning (DRL) methods use flexible neural-network-based function approximators to discover policies that generalize naturally to unseen circumstances. However, DRL methods struggle to handle the very sparse reward landscapes inherent to long-range multi-step planning situations. Here, we propose the Curious Sample Planner (CSP), which fuses elements of TAMP and DRL by combining a curiosity-guided sampling strategy with imitation learning to accelerate planning. We show that CSP can efficiently discover interesting and complex temporally-extended plans for solving a wide range of physically realistic 3D tasks. In contrast, standard planning and learning methods often fail to solve these tasks at all or do so only with a huge and highly variable number of training samples. We explore the use of a variety of curiosity metrics with CSP and analyze the types of solutions that CSP discovers. Finally, we show that CSP supports task transfer so that the exploration policies learned during experience with one task can help improve efficiency on related tasks.</p> 
### 1010.[Sparse Convex Optimization via Adaptively Regularized Hard Thresholding](https://proceedings.icml.cc/book/4251.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6264-Paper.pdf)
  Kyriakos Axiotis, Maxim Sviridenko [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6264-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6264-Supplemental.zip)
> The goal of Sparse Convex Optimization is to optimize a convex function $f$ under a sparsity constraint $s\leq s^*\gamma$, where $s^*$ is the target number of non-zero entries in a feasible solution (sparsity) and $\gamma\geq 1$ is an approximation factor. There has been a lot of work to analyze the sparsity guarantees of various algorithms (LASSO, Orthogonal Matching Pursuit (OMP), Iterative Hard Thresholding (IHT)) in terms of the Restricted Condition Number $\kappa$. The best known algorithms guarantee to find an approximate solution of value $f(x^*)+\epsilon$ with the sparsity bound of $\gamma = O\left(\kappa\min\left\{\log \frac{f(x^0)-f(x^*)}{\epsilon}, \kappa\right\}\right)$, where $x^*$ is the target solution. We present a new Adaptively Regularized Hard Thresholding (ARHT) algorithm that makes significant progress on this problem by bringing the bound down to $\gamma=O(\kappa)$, which has been shown to be tight for a general class of algorithms including LASSO, OMP, and IHT. This is achieved without significant sacrifice in the runtime efficiency compared to the fastest known algorithms. We also provide a new analysis of OMP with Replacement (OMPR) for general $f$, under the condition $s &gt; s^* \frac{\kappa^2}{4}$, which yields Compressed Sensing bounds under the Restricted Isometry Property (RIP). When compared to other Compressed Sensing approaches, it has the advantage of providing a strong tradeoff between the RIP condition and the solution sparsity, while working for any general function $f$ that meets the RIP condition.
### 1011.[On Thompson Sampling with Langevin Algorithms](https://proceedings.icml.cc/book/4252.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6280-Paper.pdf)
  Eric Mazumdar, Aldo Pacchiano, Yian Ma, Michael Jordan, Peter Bartlett [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6280-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6280-Supplemental.pdf)
> <p>Thompson sampling has been demonstrated both theoretically and empirically to enjoy favorable performance in tackling multi-armed bandits problems. Despite its successes, however, one key obstacle to its use in a much broader range of scenarios is the need for perfect samples from posterior distributions at every iteration, which is oftentimes not feasible in practice. We propose a Markov Chain Monte Carlo (MCMC) method tailored to Thompson sampling to address this issue. We construct a fast converging Langevin algorithm to generate approximate samples with accuracy guarantees. We then leverage novel posterior concentration rates to analyze the statistical risk of the overall Thompson sampling method.  Finally, we specify the necessary hyperparameters and the required computational resources for the MCMC procedure to match the optimal risk. The resulting algorithm enjoys both optimal instance-dependent frequentist regret and appealing computation complexity.</p> 
### 1012.[Strategic Classification is Causal Modeling in Disguise](https://proceedings.icml.cc/book/4253.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6286-Paper.pdf)
  John Miller, Smitha Milli, University of California Moritz Hardt [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6286-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6286-Supplemental.pdf)
> <p>Consequential decision-making incentivizes individuals to strategically adapt their behavior to the specifics of the decision rule. While a long line of work has viewed strategic adaptation as gaming and sought to mitigate its effects, recent work instead seeks to design classifiers that incentivize individuals to improve a desired quality. Key to both accounts is a cost function that dictates which adaptations are rational to undertake. In this work, we develop a causal framework for strategic adaptation. Our causal perspective clearly distinguishes between gaming and improvement and reveals an important obstacle to incentive design. We prove any procedure for designing classifiers that incentivize improvement must inevitably solve a non-trivial causal inference problem. Moreover, we show a similar result holds for designing cost functions that satisfy the requirements of previous work. With the benefit of hindsight, our results show much of the prior work on strategic classification is causal modeling in disguise.</p> 
### 1013.[Multi-fidelity Bayesian Optimization with Max-value Entropy Search and its Parallelization](https://proceedings.icml.cc/book/4254.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6289-Paper.pdf)
  Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, Masayuki Karasuyama [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6289-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6289-Supplemental.pdf)
> <p>In a standard setting of Bayesian optimization (BO), the objective function evaluation is assumed to be highly expensive. Multi-fidelity Bayesian optimization (MFBO) accelerates BO by incorporating lower fidelity observations available with a lower sampling cost. In this paper, we focus on the information-based approach, which is a popular and empirically successful approach in BO. For MFBO, however, existing information-based methods are plagued by difficulty in estimating the information gain. We propose an approach based on max-value entropy search (MES), which greatly facilitates computations by considering the entropy of the optimal function value instead of the optimal input point. We show that, in our multi-fidelity MES (MF-MES), most of additional computations, compared with usual MES, is reduced to analytical computations. Although an additional numerical integration is necessary for the information across different fidelities, this is only in one dimensional space, which can be performed efficiently and accurately. Further, we also propose parallelization of MF-MES. Since there exist a variety of different sampling costs, queries typically occur asynchronously in MFBO. We show that similar simple computations can be derived for asynchronous parallel MFBO. We demonstrate effectiveness of our approach by using benchmark datasets and a real-world application to materials science data.</p> 
### 1014.[Domain Aggregation Networks for Multi-Source Domain Adaptation](https://proceedings.icml.cc/book/4255.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6292-Paper.pdf)
  Junfeng Wen, Russell Greiner, Dale Schuurmans [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6292-Metadata.json)
> <p>In many real-world applications, we want to exploit multiple source datasets to build a model for a different but related target dataset. Despite the recent empirical success, most existing research has used ad-hoc methods to combine multiple sources, leading to a gap between theory and practice. In this paper, we develop a finite-sample generalization bound based on domain discrepancy and accordingly propose a theoretically justified optimization procedure. Our algorithm, Domain AggRegation Network (DARN), can automatically and dynamically balance between including more data to increase effective sample size and excluding irrelevant data to avoid negative effects during training. We find that DARN can significantly outperform the state-of-the-art alternatives on multiple real-world tasks, including digit/object recognition and sentiment analysis.</p> 
### 1015.[Improving Robustness of Deep-Learning-Based Image Reconstruction](https://proceedings.icml.cc/book/4256.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6293-Paper.pdf)
  Ankit Raj, Yoram Bresler, Bo Li [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6293-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6293-Supplemental.pdf)
> <p>Deep-learning-based methods for different applications have been shown vulnerable to adversarial examples. These examples make deployment of such models in safety-critical tasks questionable. Use of deep neural networks as inverse problem solvers has generated much excitement for medical imaging including CT and MRI, but recently a similar vulnerability has also been demonstrated for these tasks. We show that for such inverse problem solvers, one should analyze and study the effect of adversaries in the measurement-space, instead of the signal-space as in previous work. In this paper, we propose to modify the training strategy of end-to-end deep-learning-based inverse problem solvers to improve robustness. We introduce an auxiliary network to generate adversarial examples, which is used in a min-max formulation to build robust image reconstruction networks. Theoretically, we show for a linear reconstruction scheme the min-max formulation results in a singular-value(s) filter regularized solution, which suppresses the effect of adversarial examples occurring because of ill-conditioning in the measurement matrix. We find that a linear network using the proposed min-max learning scheme indeed converges to the same solution. In addition, for non-linear Compressed Sensing (CS) reconstruction using deep networks, we show significant improvement in robustness using the proposed approach over other methods. We complement the theory by experiments for CS on two different datasets and evaluate the effect of increasing perturbations on trained networks. We find the behavior for ill-conditioned and well-conditioned measurement matrices to be qualitatively different.</p> 
### 1016.[Outsourced Bayesian Optimization](https://proceedings.icml.cc/book/4257.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6298-Paper.pdf)
  Dmitrii Kharkovskii, Zhongxiang Dai, Bryan Kian Hsiang Low [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6298-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6298-Supplemental.pdf)
> <p>This paper presents the outsourced-Gaussian process-upper confidence bound (O-GP-UCB) algorithm, which is the first algorithm for privacy-preserving Bayesian optimization (BO) in the outsourced setting with a provable performance guarantee. We consider the outsourced setting where the entity holding the dataset and the entity performing BO are represented by different parties, and the dataset cannot be released non-privately. For example, a hospital holds a dataset of sensitive medical records and outsources the BO task on this dataset to an industrial AI company. The key idea of our approach is to make the BO performance of our algorithm similar to that of non-private GP-UCB run using the original dataset, which is achieved by using a random projection-based transformation that preserves both privacy and the pairwise distances between inputs. Our main theoretical contribution is to show that a regret bound similar to that of the standard GP-UCB algorithm can be established for our O-GP-UCB algorithm. We empirically evaluate the performance of our O-GP-UCB algorithm with synthetic and real-world datasets.</p> 
### 1017.[Learning Near Optimal Policies with Low Inherent Bellman Error](https://proceedings.icml.cc/book/4258.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6299-Paper.pdf)
  Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, Emma Brunskill [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6299-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6299-Supplemental.pdf)
> <p>We study the exploration problem with approximate linear action-value functions in episodic reinforcement learning under the notion of low inherent Bellman error, a condition normally employed to show convergence of approximate value iteration. We relate this condition to other common frameworks and show that it is strictly more general than the low rank (or linear) MDP assumption of prior work.  We provide an algorithm with a rate optimal regret bound for this setting. While computational tractability questions remain open, this enriches the class of MDPs with a linear representation for the action-value function where statistically efficient reinforcement learning is possible. </p> 
### 1018.[Message Passing Least Squares: A Unified Framework for Fast and Robust Group Synchronization](https://proceedings.icml.cc/book/4259.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6303-Paper.pdf)
  Yunpeng Shi, Gilad Lerman [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6303-Metadata.json)
> We propose an efficient algorithm for solving robust group synchronization given adversarially corrupted group ratios. We first present a theoretically guaranteed message passing algorithm that estimates the corruption levels of the group ratios. We then propose a novel weighted least squares method to estimate the group elements, where the weights are initialized using the estimated corruption levels and are iteratively updated  by incorporating cycle consistency information. We demonstrate the superior performance of our algorithm over state-of-the-art methods for $SO(3)$ synchronization using both synthetic and real data.
### 1019.[Optimal Estimator for Unlabeled Linear Regression](https://proceedings.icml.cc/book/4260.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6318-Paper.pdf)
  hang zhang, Ping Li [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6318-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6318-Supplemental.pdf)
> Unlabeled linear regression, or ``linear regression with an  unknown permutation&#x27;&#x27;,  has attracted increasing  attentions due to its applications in linkage record and de-anonymization.  However, its computation proves to be cumbersome and all existing algorithms require considerable time in the  high dimensional regime.  This paper proposes a one-step estimator which  are optimal from both the computational and statistical  sense.  From the computational perspective, our estimator  exhibits the same order of computational time  as that of the oracle case, where the covariates are known in advance and only the permutation  needs recovery.  From the statistical  perspective, when comparing with the necessary conditions for permutation recovery,  our requirement on \emph{signal-to-noise ratio} ($\snr$) agrees up to $O\bracket{\log \log n}$ difference  in certain regimes.  Numerical experiments have also been provided to  corroborate the above claims.
### 1020.[Recovery of sparse signals from a mixture of linear samples](https://proceedings.icml.cc/book/4261.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6319-Paper.pdf)
  Arya Mazumdar, Soumyabrata Pal [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6319-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6319-Supplemental.pdf)
> <p>Mixture of linear regressions is a popular learning theoretic model that is used widely to represent heterogeneous data. In the simplest form, this model assumes that the labels are generated from either of two different linear models and mixed together. Recent works of Yin et al. and Krishnamurthy et al., 2019, focus on an experimental design setting of model recovery for this problem. It is assumed that the features can be designed and queried with to obtain their label. When queried, an oracle randomly selects one of the two different sparse linear models and generates a label accordingly. How many such oracle queries are needed to recover both of the models simultaneously? This can also be though of as a generalization of the well-known compressed sensing problem (Cand`es and Tao, 2005, Donoho, 2006). In this work we address this query complexity problem and provide efficient algorithms that improves on the previously best known results. </p> 
### 1021.[Recurrent Hierarchical Topic-Guided RNN for Language Generation](https://proceedings.icml.cc/book/4262.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6326-Paper.pdf)
  Dandan Guo, Bo Chen, Ruiying Lu, Mingyuan Zhou [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6326-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6326-Supplemental.pdf)
> <p>To simultaneously capture syntax and global semantics from a text corpus, we propose a new larger-context recurrent neural network (RNN)-based language model, which extracts recurrent hierarchical semantic structure via a dynamic deep topic model to guide natural language generation. Moving beyond a conventional RNN-based language model that ignores long-range word dependencies and sentence order, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence topic dependences. For inference, we develop a hybrid of stochastic gradient Markov chain Monte Carlo and recurrent autoencoding variational Bayes. Experimental results on a variety of real-world text corpora demonstrate that the proposed model not only outperforms larger-context RNN-based language models, but also learns interpretable recurrent multilayer topics and generates diverse sentences and paragraphs that are syntactically correct and semantically coherent.</p> 
### 1022.[Predictive Coding for Locally-Linear Control](https://proceedings.icml.cc/book/4263.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6329-Paper.pdf)
  Rui Shu, Tung Nguyen, Yinlam Chow, Tuan Pham, Khoat Than, Mohammad Ghavamzadeh, Stefano Ermon, Hung Bui [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6329-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6329-Supplemental.pdf)
> <p>High-dimensional observations and unknown dynamics are major challenges when applying optimal control to many real-world decision making tasks. The Learning Controllable Embedding (LCE) framework addresses these challenges by embedding the observations into a lower dimensional latent space, estimating the latent dynamics, and then performing control directly in the latent space. To ensure the learned latent dynamics are predictive of next-observations, all existing LCE approaches decode back into the observation space and explicitly perform next-observation prediction---a challenging high-dimensional task that furthermore introduces a large number of nuisance parameters (i.e., the decoder) which are discarded during control. In this paper, we propose a novel information-theoretic LCE approach and show theoretically that explicit next-observation prediction can be replaced with predictive coding. We then use predictive coding to develop a decoder-free LCE model whose latent dynamics are amenable to locally-linear control. Extensive experiments on benchmark tasks show that our model reliably learns a controllable latent space that leads to superior performance when compared with state-of-the-art LCE baselines.</p> 
### 1023.[Near Input Sparsity Time Kernel Embeddings via Adaptive Sampling](https://proceedings.icml.cc/book/4264.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6331-Paper.pdf)
  Amir Zandieh, David Woodruff [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6331-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6331-Supplemental.pdf)
> To accelerate kernel methods, we propose a near input sparsity time method for sampling the high-dimensional space implicitly defined by a kernel transformation. Our main contribution is an importance sampling method for subsampling the feature space of a degree $q$ tensoring of data points in almost input sparsity time, improving the recent oblivious sketching of (Ahle et al., 2020) by a factor of $q^{5/2}/\epsilon^2$. This leads to a subspace embedding for the polynomial kernel as well as the Gaussian kernel with a target dimension that is only linearly dependent on the statistical dimension of the kernel and in time which is only linearly dependent on the sparsity of the input dataset. We show how our spectral matrix approximation bounds imply new statistical guarantees for kernel ridge regression. Furthermore, we empirically show that in large-scale regression tasks, our algorithm outperforms state-of-the-art kernel approximation methods.
### 1024.[Near-optimal sample complexity bounds for learning Latent $k-$polytopes and applications to Ad-Mixtures](https://proceedings.icml.cc/book/4265.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6335-Paper.pdf)
  Chiranjib Bhattacharyya, Ravindran Kannan [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6335-Metadata.json)
> Recently near-optimal bounds on sample complexity of Mixture of Gaussians was shown in the seminal paper \cite{HSNCAY18}. No such results are known for Ad-mixtures. In this paper we show that $O^*(dk/m)$ samples are sufficient to learn each of $k-$ topic vectors of LDA, a popular Ad-mixture model, with vocabulary size $d$ and $m\in \Omega(1)$ words per document, to any constant error in $L_1$ norm.  This is a corollary of the major contribution of the current paper: the first sample complexity upper bound for the problem (introduced in \cite{BK20}) of learning the vertices of a Latent $k-$ Polytope in ${\bf R}^d$, given perturbed points from it. The bound,  $O^*(dk/\beta)$, is optimal and applies to many stochastic models including LDA, Mixed Membership block Models(MMBM),Dirichlet Simplex Nest,  and large class of Ad-mixtures. The parameter, $\beta$ depends on the probability laws governing individual models and in many cases can be expressed very succintly, e.g. it is equal to the average degree of each node for MMBM, and equal to $m$ in LDA. The tightness is proved by a nearly matching lower of $\Omega^*(dk/\beta)$ by a combinatorial construction based on a code-design. Our upper bound proof combines two novel methods. The first is {\it vertex set certification} which, for any $k-$polytope $K$ gives convex geometry based sufficient conditions for a set of $k$ points from a larger candidate set to be close in Hausdorff distance to the set of $k$ vertices of the polytope. The second is {\it subset averaging} which uses $\beta$ to prove that the set of averages of all large subsets of data is a good candidate set. 
### 1025.[Population-Based Black-Box Optimization for Biological Sequence Design](https://proceedings.icml.cc/book/4266.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6338-Paper.pdf)
  Christof Angermueller, David Belanger, Andreea Gane, Zelda Mariet, David Dohan, Kevin Murphy, Lucy Colwell , D. Sculley [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6338-Metadata.json)
> <p>The use of black-box optimization for the design of new biological sequences is an emerging research area with potentially revolutionary impact. The cost and latency of wet-lab experiments requires methods that find good sequences in few experimental rounds of large batches of sequences --- a setting that off-the-shelf black-box optimization methods are ill-equipped to handle. We find that the performance of existing methods varies drastically across optimization tasks, posing a significant obstacle to real-world applications. To improve robustness, we propose population-based optimization (P3BO), which generates batches of sequences by sampling from an ensemble of methods. The number of sequences sampled from any method is proportional to the quality of sequences it previously proposed, allowing P3BO to combine the strengths of individual methods while hedging against their innate brittleness. Adapting the hyper-parameters of each of the methods online using evolutionary optimization further improves performance. Through extensive experiments on in-silico optimization tasks, we show that P3BO outperforms any single method in its population, proposing  higher quality sequences as well as more diverse batches. As such, P3BO and Adaptive-P3BO are a crucial step towards deploying ML to real-world sequence design.</p> 
### 1026.[Emergence of Separable Manifolds in Deep Language Representations](https://proceedings.icml.cc/book/4267.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6348-Paper.pdf)
  Jonathan Mamou, Hang Le, Miguel del Rio Fernandez, Cory Stephenson, Hanlin Tang, Yoon Kim, SueYeon Chung [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6348-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6348-Supplemental.pdf)
> <p>Artificial neural networks (ANNs) have shown much empirical success in solving perceptual tasks across various cognitive modalities. While they are only loosely inspired by the biological brain, recent studies report considerable similarities between  representations extracted from task-optimized ANNs and neural populations in the brain. ANNs have subsequently become a popular model class to infer computational principles underlying complex cognitive functions, and in turn, they have also emerged as a natural testbed for applying methods originally developed to probe information in neural populations. In this work, we utilize mean-field theoretic manifold analysis, a recent  technique from computational neuroscience, to analyze the high dimensional geometry of language representations from large-scale contextual embedding models. We explore representations from different model families (BERT, RoBERTa, GPT-2, etc.) and find evidence for emergence of linguistic manifolds across layer depth (e.g., manifolds for part-of-speech and combinatory categorial grammar tags). We further observe that different encoding schemes used to obtain the representations lead to differences in whether these linguistic manifolds emerge in earlier or later layers of the network. In addition, we find that the emergence of linear separability in these manifolds is driven by a combined reduction of manifolds’ radius, dimensionality and inter-manifold correlations.</p> 
### 1027.[Stochastic Hamiltonian Gradient Methods for Smooth Games](https://proceedings.icml.cc/book/4268.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6356-Paper.pdf)
  Nicolas Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent, Simon Lacoste-Julien, Ioannis Mitliagkas [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6356-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6356-Supplemental.pdf)
> <p>The analysis of smooth games has attracted attention, motivated by the success of adversarial formulations. The Hamiltonian method is a lightweight second-order approach that recasts the problem in terms of a minimization objective. Consensus optimization can be seen as a generalization: it mixes a Hamiltonian term with the original game dynamics. This family of Hamiltonian methods has shown promise in literature. However, they come with no guarantees for stochastic games. Classic stochastic extragradient and mirror-prox methods require averaging over a compact domain to achieve convergence. Recent variance-reduced first-order schemes focus on unbounded domains, but stop short of proving last-iterate convergence for bilinear matrix games. We analyze the stochastic Hamiltonian method and a novel variance-reduced variant of it and provide the first set of last-iterate convergence guarantees for stochastic unbounded bilinear games. More generally, we provide convergence guarantees for a family of stochastic games, notably including some non-convex ones. We supplement our analysis with experiments on a stochastic bilinear game, where our theory is shown to be tight, and simple adversarial machine learning formulations.</p> 
### 1028.[Understanding and Estimating the Adaptability of Domain-Invariant Representations](https://proceedings.icml.cc/book/4269.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6358-Paper.pdf)
  Ching-Yao Chuang, Antonio Torralba, Stefanie Jegelka [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6358-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6358-Supplemental.pdf)
> <p>Learning domain-invariant representations is a popular approach to unsupervised domain adaptation, i.e., generalizing from a source domain with labels to an unlabeled target domain. In this work, we aim to better understand and estimate the effect of domain-invariant representations on generalization to the target. In particular, we study the effect of the complexity of the latent, domain-invariant representation, and find that it has a significant influence on the target risk. Based on these findings, we propose a general approach for addressing this complexity tradeoff in neural networks. We also propose a method for estimating how well a model based on domain-invariant representations will perform on the target domain, without having seen any target labels. Applications of our results include model selection, deciding early stopping, and predicting the adaptability of a model between domains.</p> 
### 1029.[Adversarial Mutual Information for Text Generation](https://proceedings.icml.cc/book/4270.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6365-Paper.pdf)
  Boyuan Pan, Yazheng Yang, Kaizhao Liang, Bhavya Kailkhura, Zhongming Jin, Xian-Sheng Hua, Deng Cai, Bo Li [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6365-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6365-Supplemental.pdf)
> <p>Recent advances in maximizing mutual information (MI) between the source and target have demonstrated its effectiveness in text generation. However, previous works paid little attention to modeling the backward network of MI (i.e. dependency from the target to the source), which is crucial to the tightness of the variational information maximization lower bound. In this paper, we propose Adversarial Mutual Information (AMI): a text generation framework which is formed as a novel saddle point (min-max) optimization aiming to identify joint interactions between the source and target. Within this framework, the forward and backward networks are able to iteratively promote or demote each other's generated instances by comparing the real and synthetic data distributions. We also develop a latent noise sampling strategy that leverages random variations at the high-level semantic space to enhance the long term dependency in the generation process. Extensive experiments based on different text generation tasks demonstrate that the proposed AMI framework can significantly outperform several strong baselines, and we also show that AMI has potential to lead to a tighter lower bound of maximum mutual information for the variational information maximization problem.</p> 
### 1030.[Bidirectional Model-based Policy Optimization](https://proceedings.icml.cc/book/4271.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6366-Paper.pdf)
  Hang Lai, Jian Shen, Weinan Zhang, Yong Yu [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6366-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6366-Supplemental.zip)
> <p>Model-based reinforcement learning approaches leverage a forward dynamics model to support planning and decision making, which, however, may fail catastrophically if the model is inaccurate. Although there are several existing methods dedicated to combating the model error, the potential of the single forward model is still limited. In this paper, we propose to additionally construct a backward dynamics model to reduce the reliance on accuracy in forward model predictions. We develop a novel method, called Bidirectional Model-based Policy Optimization (BMPO) to utilize both the forward model and backward model to generate short branched rollouts for policy optimization. Furthermore, we theoretically derive a tighter bound of return discrepancy, which shows the superiority of BMPO against the one using merely the forward model. Extensive experiments demonstrate that BMPO outperforms state-of-the-art model-based methods in terms of sample efficiency and asymptotic performance.</p> 
### 1031.[Input-Sparsity Low Rank Approximation in Schatten Norm](https://proceedings.icml.cc/book/4272.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6381-Paper.pdf)
  Yi Li, David Woodruff [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6381-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6381-Supplemental.zip)
> We give the first input-sparsity time algorithms for the rank-$k$ low rank approximation problem in every Schatten norm. Specifically, for a given $n\times n$ matrix $A$, our algorithm computes $Y,Z\in \R^{n\times k}$, which, with high probability, satisfy $\|A-YZ^T\|_p \leq (1+\eps)\|A-A_k\|_p$, where $\|M\|_p = \left (\sum_{i=1}^n \sigma_i(M)^p \right )^{1/p}$ is the Schatten $p$-norm of a matrix $M$ with singular values $\sigma_1(M), \ldots, \sigma_n(M)$, and where $A_k$ is the best rank-$k$ approximation to $A$. Our algorithm runs in time $\tilde{O}(\nnz(A) + n^{\alpha_p}\poly(k/\eps))$, where $\alpha_p = 1$ for $p\in [1,2)$ and $\alpha_p = 1 + (\omega-1)(1-2/p)$ for $p&gt;2$ and $\omega \approx 2.374$ is the exponent of matrix multiplication. For the important case of $p = 1$, which corresponds to the more ``robust&#x27;&#x27; nuclear norm, we obtain $\tilde{O}(\nnz(A) + n \cdot \poly(k/\epsilon))$ time, which was previously only known for the Frobenius norm $(p = 2)$. Moreover, since $\alpha_p &lt; \omega$ for every $p$, our algorithm has a better dependence on $n$ than that in the singular value decomposition for every $p$. Crucial to our analysis is the use of dimensionality reduction for Ky-Fan $p$-norms. 
### 1032.[Do We Need Zero Training Loss After Achieving Zero Training Error?](https://proceedings.icml.cc/book/4273.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6392-Paper.pdf)
  Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, Masashi Sugiyama [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6392-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6392-Supplemental.pdf)
> <p>Overparameterized deep networks have the capacity to memorize training data with zero training error.  Even after memorization, the training loss continues to approach zero, making the model overconfident and the test performance degraded.  Since existing regularizers do not directly aim to avoid zero training loss, they often fail to maintain a moderate level of training loss, ending up with a too small or too large loss.  We propose a direct solution called \emph{flooding} that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the \emph{flooding level}.  Our approach makes the loss float around the flooding level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the flooding level.  This can be implemented with one line of code, and is compatible with any stochastic optimizer and other regularizers.  With flooding, the model will continue to ``random walk'' with the same non-zero training loss, and we expect it to drift into an area with a flat loss landscape that leads to better generalization.  We experimentally show that flooding improves performance and as a byproduct, induces a double descent curve of the test loss.</p> 
### 1033.[Learning and sampling of atomic interventions from observations](https://proceedings.icml.cc/book/4274.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6395-Paper.pdf)
  Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, Ashwin Maran, Vinodchandran N. Variyam [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6395-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6395-Supplemental.pdf)
> <p>We study the problem of efficiently estimating the effect of an intervention on a single variable using observational samples. Our goal is to give algorithms with polynomial time and sample complexity in a non-parametric setting.</p>  <p>Tian and Pearl (AAAI '02) have exactly characterized the class of causal graphs for which causal effects of atomic interventions can be identified from observational data. We make their result quantitative.  Suppose 𝒫 is a causal model on a set V of n observable variables with respect to a given causal graph G,  and let do(x) be an identifiable intervention on a variable X.  We show that assuming that G has bounded in-degree and bounded c-components and that the observational distribution satisfies a strong positivity condition:</p>  <p>(i) [Evaluation] There is an algorithm that outputs with probability 2/3 an evaluator for a distribution P^ that satisfies TV(P(V | do(x)), P^(V)) &lt; eps using m=O~(n/eps^2) samples from P and O(mn) time. The evaluator can return in O(n) time the probability P^(v) for any assignment v to V.</p>  <p>(ii) [Sampling] There is an algorithm that outputs with probability 2/3 a sampler for a distribution P^ that satisfies TV(P(V | do(x)), P^(V)) &lt; eps using m=O~(n/eps^2) samples from P and O(mn) time. The sampler returns an iid sample from P^ with probability 1-delta in O(n log(1/delta)/eps) time. </p>  <p>We extend our techniques to estimate P(Y | do(x)) for a subset Y of variables of interest. We also show lower bounds for the sample complexity, demonstrating that our sample complexity has optimal dependence on the parameters n and eps as well as the strong positivity parameter.</p> 
### 1034.[Understanding and Mitigating the Tradeoff between Robustness and Accuracy](https://proceedings.icml.cc/book/4275.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6401-Paper.pdf)
  Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, Percy Liang [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6401-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6401-Supplemental.pdf)
> <p>Adversarial training augments the training set with perturbations to improve the robust error (over worst-case perturbations), but it often leads to an increase in the standard error (on unperturbed test inputs). Previous explanations for this tradeoff rely on the assumption that no predictor in the hypothesis class has low standard and robust error. In this work, we precisely characterize the effect of augmentation on the standard error in linear regression when the optimal linear predictor has zero standard and robust error. In particular, we show that the standard error could increase even when the augmented perturbations have noiseless observations from the optimal linear predictor. We then prove that the recently proposed robust self-training (RST) estimator improves robust error without sacrificing standard error for noiseless linear regression. Empirically, for neural networks, we find that RST with different adversarial training methods improves both standard and robust error for random and adversarial rotations and adversarial l_infty perturbations in CIFAR-10.</p> 
### 1035.[Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction](https://proceedings.icml.cc/book/4276.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6414-Paper.pdf)
  Filipe de Avila Belbute-Peres, Thomas Economon, Zico Kolter [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6414-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6414-Supplemental.pdf)
> <p>Solving large complex partial differential equations (PDEs), such as those that arise in computational fluid dynamics (CFD), is a computationally expensive process. This has motivated the use of deep learning approaches to approximate the PDE solutions, yet the simulation results predicted from these approaches typically do not generalize well to truly novel scenarios. In this work, we develop a hybrid (graph) neural network that combines a traditional graph convolutional network with an embedded differentiable fluid dynamics simulator inside the network itself. By combining an actual CFD simulator (run on a much coarser resolution representation of the problem) with the graph network, we show that we can both generalize well to new situations and benefit from the substantial speedup of neural network CFD predictions, while also substantially outperforming the coarse CFD simulation alone.</p> 
### 1036.[From ImageNet to Image Classification: Contextualizing Progress on Benchmarks](https://proceedings.icml.cc/book/4277.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6430-Paper.pdf)
  Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Andrew Ilyas, Aleksander Madry [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6430-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6430-Supplemental.pdf)
> <p>Creating machine learning datasets often necessitates the use of automated data retrieval and crowdsourced annotation, giving rise to an inevitably noisy pipeline. We perform large-scale human studies to investigate the impact of such a pipeline on ImageNet---one of the key datasets driving progress in computer vision. We find that seemingly innocuous design choices (e.g., exact task setup, filtering procedure, annotators employed) can have an unexpected impact on the resulting dataset---including the introduction of spurious correlations that state-of-the-art models exploit. Overall, our results highlight a misalignment between the way we train our models and the task we actually expect them to solve, emphasizing the need for fine-grained evaluation techniques that go beyond average-case accuracy.</p> 
### 1037.[On Implicit Regularization in $\beta$-VAEs](https://proceedings.icml.cc/book/4278.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6431-Paper.pdf)
  Abhishek Kumar, Ben Poole [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6431-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6431-Supplemental.pdf)
> While the impact of variational inference (VI) on posterior inference in a fixed generative model is well-characterized, its role in regularizing a learned generative model when used in variational autoencoders (VAEs) is poorly understood. We study the regularizing effects of variational distributions on learning in generative models from two perspectives. First, we analyze the role that the choice of variational family plays in imparting uniqueness to the learned model by restricting the set of optimal generative models. Second, we study the regularization effect of the variational family on the local geometry of the decoding model. This analysis uncovers the regularizer implicit in the $\beta$-VAE objective, and leads to an approximation consisting of a deterministic autoencoding objective plus analytic regularizers that depend on the Hessian or Jacobian of the decoding model, unifying VAEs with recent heuristics proposed for training regularized autoencoders. We empirically verify these findings, observing that the proposed deterministic objective indeed exhibits similar behavior to the $\beta$-VAE in terms of objective value and sample quality on CelebA and MNIST.  
### 1038.[Data Amplification: Instance-Optimal Property Estimation ](https://proceedings.icml.cc/book/4279.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6444-Paper.pdf)
  Yi Hao, Alon Orlitsky [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6444-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6444-Supplemental.pdf)
> The best-known and most commonly used technique for distribution-property estimation uses a plug-in estimator, with empirical frequency replacing the underlying distribution. We present novel linear-time-computable estimators that significantly ``amplify&#x27;&#x27; the effective amount of data available. For a large variety of distribution properties including four of the most popular ones and for every underlying distribution, they achieve the accuracy that the empirical-frequency plug-in estimators would attain using a logarithmic-factor more samples. Specifically, for Shannon entropy and a broad class of Lipschitz properties including the $L_1$ distance to a fixed distribution, the new estimators use $n$ samples to achieve the accuracy attained by the empirical estimators with $n\log n$ samples. For support-size and coverage, the new estimators use $n$ samples to achieve the performance of empirical frequency with sample size $n$ times the logarithm of the property value. Significantly strengthening the traditional min-max formulation, these results hold not only for the worst distributions, but for each and every underlying distribution. Furthermore, the logarithmic amplification factors are optimal. Experiments on a wide variety of distributions show that the new estimators outperform the previous state-of-the-art estimators designed for each specific property.  
### 1039.[Provable guarantees for decision tree induction: the agnostic setting ](https://proceedings.icml.cc/book/4280.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6447-Paper.pdf)
  Guy Blanc, Jane Lange, Li-Yang Tan [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6447-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6447-Supplemental.pdf)
> We give strengthened provable guarantees on the performance of widely employed and empirically successful {\sl top-down decision tree learning heuristics}.  While prior works have focused on the realizable setting, we consider the more realistic and challenging {\sl agnostic} setting.  We show that for all monotone functions $f$ and $s\in \N$, these heuristics construct a decision tree  of size $s^{\tilde{O}((\log s)/\eps^2)}$ that achieves error $\le \opt_s + \eps$, where $\opt_s$ denotes the error of the optimal size-$s$ decision tree for $f$.  Previously such a guarantee was not known to be achievable by any algorithm, even one that is not based on top-down heuristics.  We complement our algorithmic guarantee with a near-matching $s^{\tilde{\Omega}(\log s)}$ lower bound. 
### 1040.[Statistical Bias in Dataset Replication](https://proceedings.icml.cc/book/4281.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6461-Paper.pdf)
  Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Jacob Steinhardt, Aleksander Madry [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6461-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6461-Supplemental.pdf)
> <p>Dataset replication is a useful tool for assessing whether models have overfit to a specific validation set or the exact circumstances under which it was generated. In this paper, we highlight the importance of statistical modeling in dataset replication: we present unintuitive yet pervasive ways in which statistical bias, when left unmitigated, can skew results.  Specifically, we examine ImageNet-v2, a replication of the ImageNet dataset that induces a significant drop in model accuracy, presumed to be caused by a benign distribution shift between the datasets. We show, however, that by identifying and accounting for the aforementioned bias, we can explain the vast majority of this accuracy drop. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication.</p> 
### 1041.[Towards Adaptive Residual Network Training: A Neural-ODE Perspective](https://proceedings.icml.cc/book/4282.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6462-Paper.pdf)
  chengyu dong, Liyuan Liu, Zichao Li, Jingbo Shang [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6462-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6462-Supplemental.pdf)
> <p>Serving as a crucial factor, the depth of residual networks balances model capacity, performance, and training efficiency. However, depth has been long fixed as a hyper-parameter and needs laborious tuning, due to the lack of theories describing its dynamics. Here, we conduct theoretical analysis on network depth and introduce adaptive residual network training, which gradually increases model depth during training. Specifically, from an ordinary differential equation perspective, we describe the effect of depth growth with embedded errors, characterize the impact of model depth with truncation errors, and derive bounds for them. Illuminated by these derivations, we propose an adaptive training algorithm for residual networks, LipGrow, which automatically increases network depth and accelerates model training. In our experiments, it achieves better or comparable performance while reducing ~50% of training time.</p> 
### 1042.[Overparameterization hurts worst-group accuracy with spurious correlations](https://proceedings.icml.cc/book/4283.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6478-Paper.pdf)
  Shiori Sagawa, aditi raghunathan, Pang Wei Koh, Percy Liang [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6478-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6478-Supplemental.pdf)
> <p>Increasing model capacity well beyond the point of zero training error has been observed to improve average test accuracy. However, such overparameterized models have been recently shown to obtain low worst-group accuracy --- i.e., low accuracy on atypical groups of test examples --- when there are spurious correlations that hold for the majority of training examples. We show on two image datasets that in contrast to average accuracy, overparameterization hurts worst-group accuracy in the presence of spurious correlations. We replicate this surprising phenomenon in a synthetic example and identify properties of the data distribution that induce the detrimental effect of overparameterization on worst-group accuracy. Our analysis leads us to show that a counter-intuitive approach of subsampling the majority group yields high worst-group accuracy in the overparameterized regime, whereas upweighting the minority does not. Our results suggest that when it comes to achieving high worst-group accuracy, there is a tension between using overparameterized models vs. using all of the training data.</p> 
### 1043.[A Nearly-Linear Time Algorithm for Exact Community Recovery in Stochastic Block Model](https://proceedings.icml.cc/book/4284.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6486-Paper.pdf)
  Peng Wang, Zirui Zhou, Anthony Man-Cho So [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6486-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6486-Supplemental.pdf)
> Learning community structures in graphs that are randomly generated by stochastic block models (SBMs) has received much attention lately. In this paper, we focus on the problem of exactly recovering the communities in a binary symmetric SBM, where a graph of $n$ vertices is partitioned into two equal-sized communities and the vertices are connected with probability $p = \alpha\log(n)/n$ within communities and $q = \beta\log(n)/n$ across communities for some $\alpha&gt;\beta&gt;0$. We propose a two-stage iterative algorithm for solving this problem, which employs the power method with a random starting point in the first-stage and turns to a generalized power method that can identify the communities in a finite number of iterations in the second-stage. It is shown that for any fixed $\alpha$ and $\beta$ such that $\sqrt{\alpha} - \sqrt{\beta} &gt; \sqrt{2}$, which is known to be the information-theoretical limit for exact recovery, the proposed algorithm exactly identifies the underlying communities in $\tilde{O}(n)$ running time with probability tending to one as $n\rightarrow\infty$. As far as we know, this is the first algorithm with nearly-linear running time that achieves exact recovery at the information-theoretical limit. We also present numerical results of the proposed algorithm to support and complement our theoretical development.
### 1044.[Online Multi-Kernel Learning with Graph-Structured Feedback](https://proceedings.icml.cc/book/4285.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6490-Paper.pdf)
  Pouya M Ghari, Yanning Shen [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6490-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6490-Supplemental.pdf)
> <p>Multi-kernel learning (MKL) exhibits reliable performance in nonlinear function approximation tasks. Instead of using one kernel, it learns the optimal kernel from a pre-selected dictionary of kernels.  The selection of the dictionary has crucial impact on both the performance and complexity of MKL. Specifically, inclusion of a large number of irrelevant kernels may impair the accuracy, and increase the complexity of MKL algorithms. To enhance the accuracy, and alleviate the computational burden, the present paper develops a novel scheme which actively chooses relevant kernels. The proposed framework models the pruned kernel combination as feedback collected from a graph, that is refined 'on the fly.' Leveraging the random feature approximation, we propose an online scalable multi-kernel learning approach with graph feedback, and prove that the proposed algorithm enjoys sublinear regret. Numerical tests on real datasets demonstrate the effectiveness of the novel approach.</p> 
### 1045.[Is Local SGD Better than Minibatch SGD?](https://proceedings.icml.cc/book/4286.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6502-Paper.pdf)
  Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, H. Brendan McMahan, Ohad Shamir, Nati Srebro [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6502-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6502-Supplemental.pdf)
> <p>We study local SGD (also known as parallel SGD and federated SGD), a natural and frequently used distributed optimization method. Its theoretical foundations are currently lacking and we highlight how all existing error guarantees in the convex setting are dominated by a simple baseline, minibatch SGD. (1) For quadratic objectives we prove that local SGD strictly dominates minibatch SGD and that accelerated local SGD is minmax optimal for quadratics; (2) For general convex objectives we provide the first guarantee that at least \emph{sometimes} improves over minibatch SGD, but our guarantee does not always improve over, nor even match, minibatch SGD; (3) We show that indeed local SGD does \emph{not} dominate minibatch SGD by presenting a lower bound on the performance of local SGD that is worse than the minibatch SGD guarantee.</p> 
### 1046.[On Lp-norm Robustness of Ensemble Decision Stumps and Trees](https://proceedings.icml.cc/book/4287.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6513-Paper.pdf)
  Yihan Wang, Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6513-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6513-Supplemental.pdf)
> Recent papers have demonstrated that ensemble stumps and trees could be vulnerable to small input perturbations, so robustness verification and defense for those models have become an important research problem. However, due to the structure of decision trees, where each node makes decision purely based on one feature value, all the previous works only consider the $\ell_\infty$ norm perturbation. To study robustness with respect to a general $\ell_p$ norm perturbation, one has to consider correlation between perturbations on different features, which has not been handled by previous algorithms. In this paper, we study the robustness verification and defense with respect to general $\ell_p$ norm perturbation for ensemble trees and stumps. For robustness verification, we prove that exact verification is NP-complete for $p\in(0, \infty)$ while polynomial time algorithms exist for $p=0$ or $\infty$. Approximation algorithms based on dynamic programming is then developed for verifying ensemble trees and stumps. For robustness training, we propose the first certified defense method for training ensemble stumps and trees with respect to $\ell_p$ norm perturbations. The effectiveness of proposed algorithms is verified empirically on real datasets. 
### 1047.[Sub-linear Memory Sketches for Near Neighbor Search on Streaming Data with RACE](https://proceedings.icml.cc/book/4288.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6525-Paper.pdf)
  Benjamin Coleman, Anshumali Shrivastava, Richard Baraniuk [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6525-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6525-Supplemental.pdf)
> <p>We present the first sublinear memory sketch that can be queried to find the nearest neighbors in a dataset. Our online sketching algorithm compresses an N element dataset to a sketch of size O(N^b log^3 N) in O(N^(b+1) log^3 N) time, where b &lt; 1. This sketch can correctly report the nearest neighbors of any query that satisfies a stability condition parameterized by b. We achieve sublinear memory performance on stable queries by combining recent advances in locality sensitive hash (LSH)-based estimators, online kernel density estimation, and compressed sensing. Our theoretical results shed new light on the memory-accuracy tradeoff for nearest neighbor search, and our sketch, which consists entirely of short integer arrays, has a variety of attractive features in practice. We provide rigorous theoretical guarantees and present a thorough evaluation of the memory-recall tradeoff of our method on a friend recommendation task in social media networks, including the Google plus graph. We find that RACE provides orders of magnitude better compression than the random projection based alternative while retaining the ability to report the nearest neighbors of practical queries. We expect that our theory will lead to new insight on geometric sketching problems while our methods will enable new possibilities in high-speed data mining and IoT settings. </p> 
### 1048.[Understanding Self-Training for Gradual Domain Adaptation](https://proceedings.icml.cc/book/4289.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6529-Paper.pdf)
  Ananya Kumar, Tengyu Ma, Percy Liang [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6529-Metadata.json)
> <p>Machine learning systems must adapt to data distributions that evolve over time, in applications ranging from sensor networks and self-driving car perception modules to brain-machine interfaces. We consider gradual domain adaptation, where the goal is to adapt an initial classifier trained on a source domain given only unlabeled data that shifts gradually in distribution towards a target domain. We prove the first non-vacuous upper bound on the error of self-training with gradual shifts, under settings where directly adapting to the target domain can result in unbounded error. The theoretical analysis leads to algorithmic insights, highlighting that regularization and label sharpening are essential even when we have infinite data, and suggesting that self-training works particularly well for shifts with small Wasserstein-infinity distance. Leveraging the gradual shift structure leads to higher accuracies on a rotating MNIST dataset and a realistic Portraits dataset.</p> 
### 1049.[Concept Bottleneck Models](https://proceedings.icml.cc/book/4290.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6530-Paper.pdf)
  Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, Percy Liang [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6530-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6530-Supplemental.pdf)
> <p>We seek to learn models that support interventions on high-level concepts: would the model predict severe arthritis if it thought there was a bone spur in the x-ray? State-of-the-art models today do not typically support manipulation of concepts like "the existence of bone spurs", as they are trained end-to-end from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts (provided at training time), and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts ("bone spurs") and bird attributes ("wing color"). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.</p> 
### 1050.[Optimal Bounds between f-Divergences and Integral Probability Metrics](https://proceedings.icml.cc/book/4291.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6534-Paper.pdf)
  Rohit Agrawal, Thibaut Horel [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6534-Metadata.json)
> <p>The families of f-divergences (e.g. the Kullback-Leibler divergence) and Integral Probability Metrics (e.g. total variation distance or maximum mean discrepancies) are commonly used in optimization and estimation. In this work, we systematically study the relationship between these two families from the perspective of convex duality. Starting from a tight variational representation of the f-divergence, we derive a generalization of the moment generating function, which we show exactly characterizes the best lower bound of the f-divergence as a function of a given IPM. Using this characterization, we obtain new bounds on IPMs defined by classes of unbounded functions, while also recovering in a unified manner well-known results for bounded and subgaussian functions (e.g. Pinsker's inequality and Hoeffding's lemma).</p> 
### 1051.[Robustness to Spurious Correlations via Human Annotations](https://proceedings.icml.cc/book/4292.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6535-Paper.pdf)
  Megha Srivastava, Tatsunori Hashimoto, Percy Liang [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6535-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6535-Supplemental.pdf)
> <p>The reliability of machine learning systems critically assumes that the associations between features and labels remain similar between training and test distributions. However, unmeasured variables, such as confounders, break this assumption---useful correlations between features and labels at training time can become useless or even harmful at test time. For example, high obesity is generally predictive for heart disease, but this relation may not hold for smokers who generally have lower rates of obesity and higher rates of heart disease. We present a framework for making models robust to spurious correlations by leveraging humans' common sense knowledge of causality. Specifically, we use human annotation to augment each training example with a potential unmeasured variable (i.e. an underweight patient with heart disease may be a smoker), which reduces the problem to a covariate shift problem. We then introduce a new distributionally robust optimization objective over unmeasured variables (UV-DRO) to control the worst-case loss over possible test-time shifts. Empirically, we show 5--10% improvements on a digit recognition task confounded by rotation, and 1.5--5% gains on the task of predicting arrests from NYPD Police Stops confounded by location.</p> 
### 1052.[DROCC: Deep Robust One-Class Classification](https://proceedings.icml.cc/book/4293.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6556-Paper.pdf)
  Sachin Goyal, Aditi Raghunathan, Moksh Jain, Harsha Vardhan Simhadri, Prateek Jain [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6556-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6556-Supplemental.pdf)
> <p>Classical approaches for one-class problems such as one-class SVM (Schölkopf et al., 1999) and isolation forest (Liu et al., 2008) require careful feature engineering when applied to structured domains like images. To alleviate this concern, state-of-the-art methods like DeepSVDD (Ruff et al., 2018) consider the natural alternative of minimizing a classical one -class loss applied to the learned final layer representations. However, such an approach suffers from the fundamental drawback that a representation that simply collapses all the inputs minimizes the one class loss; heuristics to mitigate collapsed representations provide limited benefits. In this work, we propose Deep Robust One Class Classification (DROCC) method that is robust to such a collapse by training the network to distinguish the training points from their perturbations, generated adversarially. DROCC is motivated by the assumption that the interesting class lies on a locally linear low dimensional manifold. Empirical evaluation demonstrates DROCC’s effectiveness on two different one-class problem settings and on a range of real-world datasets across different domains—images (CIFAR and ImageNet), audio and timeseries, offering up to 20% increase in accuracy over the state-of-the-art in anomaly detection.</p> 
### 1053.[Efficiently Solving MDPs with Stochastic Mirror Descent](https://proceedings.icml.cc/book/4294.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6568-Paper.pdf)
  Yujia Jin, Aaron Sidford [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6568-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6568-Supplemental.pdf)
> <p>In this paper we present a unified framework based on primal-dual stochastic mirror descent for approximately solving infinite-horizon Markov decision processes (MDPs) given a generative model. When applied to an average-reward MDP with \A total actions and mixing time bound \tmix our method computes an \eps-optimal policy with an expected \Otil(\tmix^2 \A \eps^{-2}) samples from the state-transition matrix, removing the ergodicity dependence of prior art.  When applied to a \gamma-discounted MDP with A total actions our method computes an  eps-optimal policy with an expected \Otil((1-\gamma)^{-4} \A \eps^{-2}) samples, improving over the best-known primal-dual methods while matching the state-of-the-art up to a (1-\gamma)^{-1} factor. Both methods are model-free, update state values and policy simultaneously, and run in time linear in the number of samples taken.</p> 
### 1054.[Handling the Positive-Definite Constraint in the Bayesian Learning Rule](https://proceedings.icml.cc/book/4295.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6575-Paper.pdf)
  Wu Lin, Mark Schmidt, Mohammad Emtiyaz Khan [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6575-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6575-Supplemental.pdf)
> <p>Bayesian learning rule is a recently proposed variational inference method, which not only contains many existing learning algorithms as special cases but also enables the design of new algorithms. Unfortunately, when posterior parameters lie in an open constraint set, the rule may not satisfy the constraints and require line-searches which could slow down the algorithm. In this paper, we fix this issue for the positive-definite constraint by proposing an improved rule that naturally handles the constraint. Our modification is obtained using Riemannian gradient methods, and is valid when the approximation attains a block-coordinate natural parameterization (e.g., Gaussian distributions and their mixtures). Our method outperforms existing methods without any significant increase in computation.  Our work makes it easier to apply the learning rule in presence of positive-definite constraints in parameter spaces.</p> 
### 1055.[A simpler approach to accelerated optimization: iterative averaging meets optimism](https://proceedings.icml.cc/book/4296.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6589-Paper.pdf)
  Pooria Joulani, Anant Raj, András György, Csaba Szepesvari [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6589-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6589-Supplemental.pdf)
> <p>Recently there have been several attempts to improve the rates of convergence achievable for smooth objectives. In particular, several recent papers have attempted to extend Nesterov's accelerated algorithm to stochastic and variance-reduced optimization. In this paper, we show that there is a simpler approach to obtaining accelerated rates: applying generic, well-known optimistic online learning algorithms and using the online average of their predictions to query the (deterministic or stochastic) first-order optimization oracle at each time step. In particular, we tighten the recent results of Cutkosky (2019) to demonstrate theoretically that online averaging results in a reduced optimization gap, independently of the algorithm involved. Then, we show that a simple tuning of existing generic optimistic online learning algorithms (e.g., Joulani et al [2017]), when combined with the reduced error quantified above, naturally results in optimal accelerated rates. \todoc{what would you consider unnatural? get rid of this word?} Importantly, the smooth objective may or may not be strongly-convex, and the rates are nevertheless optimal for both stochastic and deterministic first-order oracles.  We further show that the same ideas transfer to variance-reduced optimization. In each case, the proofs are much simpler than the previous work, such as the new derivations of accelerated algorithms based on a primal-dual view (Wang and Abernethy, 2018) or the ideas based on linear coupling (Allen-Zhu and Orecchia, 2017). Importantly, we also provide algorithms that maintain the ``universality'' property, meaning that the same algorithm achieves the optimal rate for smooth and non-smooth objectives without further prior knowledge, generalizing the results of Kavis et al (2019) and solving a number of their open problems.</p> 
### 1056.[Training Binary Neural Networks using the Bayesian Learning Rule](https://proceedings.icml.cc/book/4297.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6594-Paper.pdf)
  Xiangming Meng, Roman Bachmann, Mohammad Emtiyaz Khan [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6594-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6594-Supplemental.pdf)
> <p>Neural networks with binary weights are computation-efficient and hardware-friendly, but their training is challenging because it involves a discrete optimization problem. Surprisingly, ignoring the discrete nature of the problem and using gradient-based methods, such as Straight-Through Estimator, still works well in practice. This raises the question: are there principled approaches which justify such methods? In this paper, we propose such an approach using the Bayesian learning rule. The rule, when applied to estimate a Bernoulli distribution over the binary weights, results in an algorithm which justifies some of the algorithmic choices made by the previous approaches. The algorithm not only obtains state-of-the-art performance, but also enables uncertainty estimation and continual learning to avoid catastrophic forgetting. Our work provides a principled approach for training binary neural networks which also justifies and extends existing approaches. </p> 
### 1057.[High-dimensional Robust Mean Estimation via Gradient Descent](https://proceedings.icml.cc/book/4298.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6611-Paper.pdf)
  Yu Cheng, Ilias Diakonikolas, Rong Ge, Mahdi Soltanolkotabi [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6611-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6611-Supplemental.pdf)
> <p>We study the problem of high-dimensional robust mean estimation in the presence of a constant fraction of adversarial outliers. A recent line of work has provided sophisticated polynomial-time algorithms for this problem with dimension-independent error guarantees for a range of natural distribution families. In this work, we show that a natural non-convex formulation of the problem can be solved directly by gradient descent. Our approach leverages a novel structural lemma, roughly showing that any approximate stationary point of our non-convex objective gives a near-optimal solution to the underlying robust estimation task. Our work establishes an intriguing connection between algorithmic high-dimensional robust statistics and non-convex optimization, which may have broader applications to other robust estimation tasks.</p> 
### 1058.[From Chaos to Order: Symmetry and Conservation Laws in Game Dynamics](https://proceedings.icml.cc/book/4299.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6612-Paper.pdf)
  Sai Ganesh Nagarajan, David Balduzzi, Georgios Piliouras [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6612-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6612-Supplemental.zip)
> <p>Games are an increasingly useful tool for training and testing learning algorithms. Recent examples include GANs, AlphaZero and the AlphaStar league. However, multi-agent learning can be extremely difficult to predict and control. Even simple games learning dynamics can yield chaotic behavior.  In this paper, we present basic \emph{mechanism design} tools for constructing games with predictable and controllable dynamics. We show that arbitrarily large and complex network games, encoding both cooperation (team play) and competition (zero-sum interaction), exhibit conservation laws when agents use the standard regret-minimizing dynamics known as Follow-the-Regularized-Leader. These laws persist even if different agents use different dynamics and encode long-range correlations between agents' behavior even though the agents may not interact directly. Moreover, we provide sufficient conditions under which the dynamics have multiple, linearly independent, conservation laws. Increasing the number of conservation laws results in more predictable dynamics, eventually making chaotic behavior in some cases even formally impossible.</p> 
### 1059.[Hierarchically Decoupled Morphological Transfer](https://proceedings.icml.cc/book/4300.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6616-Paper.pdf)
  Donald Hejna, Lerrel Pinto, Pieter Abbeel [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6616-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6616-Supplemental.pdf)
> <p>Learning long-range behaviors on complex high-dimensional agents is a fundamental problem in robot learning. For such tasks, we argue that transferring learned information from a morphologically simpler agent can massively improve the sample efficiency of a more complex one. To this end, we propose a hierarchical decoupling of policies into two parts: an independently learned low-level policy and a transferable high-level policy. To remedy poor transfer performance due to mismatch in morphologies, we contribute two key ideas. First, we show that incentivizing a complex agent's low-level to imitate a simpler agent's low-level significantly improves zero-shot high-level transfer. Second, we show that KL-regularized training of the high level stabilizes learning and prevents mode-collapse. Finally, on a suite of navigation and manipulation environments, we demonstrate the applicability of hierarchical transfer on long-range tasks across morphologies.</p> 
### 1060.[Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup](https://proceedings.icml.cc/book/4301.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6618-Paper.pdf)
  Jang-Hyun Kim, Wonho Choo, Hyun Oh Song [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6618-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6618-Supplemental.pdf)
> <p>While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets.</p> 
### 1061.[Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](https://proceedings.icml.cc/book/4302.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6626-Paper.pdf)
  Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, Joseph Gonzalez [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6626-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6626-Supplemental.pdf)
> <p>Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.</p> 
### 1062.[Interpolation between CNNs and ResNets](https://proceedings.icml.cc/book/4303.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6627-Paper.pdf)
  Zonghan Yang, Yang Liu, Chenglong Bao, Zuoqiang Shi [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6627-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6627-Supplemental.pdf)
> <p>Although ordinary differential equations (ODEs) provide insights for designing networks architectures, its relationship with the non-residual convolutional neural networks (CNNs) is still unclear. In this paper, we present a novel ODE model by adding a damping term. It can be shown that the proposed model can recover both a ResNet and a CNN by adjusting an interpolation coefficient. Therefore, the damped ODE model provides a unified framework for the interpretation of CNNs and ResNets. The Lyapunov analysis reveals better stability of the proposed model, and thus yields robustness improvement of the learned networks. Experiments on a number of image classification benchmarks show that the proposed model substantially improves the accuracy of ResNet and ResNeXt over the perturbed inputs from both stochastic noise and adversarial attack methods. Moreover, the loss landscape analysis demonstrates the improved robustness of our method along the attack direction.</p> 
### 1063.[Online metric algorithms with untrusted predictions](https://proceedings.icml.cc/book/4304.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6657-Paper.pdf)
  Antonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, Bertrand Simon [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6657-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6657-Supplemental.pdf)
> <p>Machine-learned predictors, although achieving very good results for inputs resembling training data, cannot possibly provide perfect predictions in all situations. Still, decision-making systems that are based on such predictors need not only to benefit from good predictions but also to achieve a decent performance when the predictions are inadequate. In this paper, we propose a prediction setup for Metrical Task Systems (MTS), a broad class of online decision-making problems including, e.g., caching, k-server and convex body chasing. We utilize results from the theory of online algorithms to show how to make the setup robust. We extend our setup in two ways, (1) adapting it beyond MTS to the online matching on the line problem, and (2) specifically for caching, slightly enriching the predictor’s output to achieve an improved dependence on the prediction error. Finally, we present an empirical evaluation of our methods on real world datasets, which suggests practicality.</p> 
### 1064.[Collaborative Machine Learning with Incentive-Aware Model Rewards](https://proceedings.icml.cc/book/4305.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6668-Paper.pdf)
  Rachael Hwee Ling Sim, Yehong Zhang, Bryan Kian Hsiang Low, Mun Choon Chan [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6668-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6668-Supplemental.pdf)
> <p>Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party's contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party's contribution based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy minimum fairness and trade off between the desirable properties via an adjustable parameter. The value of each party's model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.</p> 
### 1065.[On Convergence-Diagnostic based Step Sizes for Stochastic Gradient Descent](https://proceedings.icml.cc/book/4306.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6674-Paper.pdf)
  Scott Pesme, Aymeric Dieuleveut, Nicolas Flammarion [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6674-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6674-Supplemental.zip)
> <p>Constant step-size Stochastic Gradient Descent exhibits two phases: a transient phase during which iterates make fast progress towards the optimum, followed by a stationary phase during which iterates oscillate around the optimal point. In this paper, we show that efficiently detecting this transition and appropriately decreasing the step size can lead to fast convergence rates. We analyse the classical statistical test proposed by Pflug (1983), based on the inner product between consecutive stochastic gradients. Even in the simple case where the objective function is quadratic we show that this test cannot lead to an adequate convergence diagnostic. We propose then a novel and simple statistical procedure that accurately detects stationarity and we provide experimental results showing state-of-the-art performance on synthetic and real-word datasets.</p> 
### 1066.[Equivariant Flows: exact likelihood generative learning for symmetric densities.](https://proceedings.icml.cc/book/4307.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6711-Paper.pdf)
  Jonas Köhler, Leon Klein, Frank Noe [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6711-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6711-Supplemental.zip)
> <p>Normalizing flows  are  exact-likelihood  generative  neural  networks  which  approximately transform  samples from a simple prior distribution to samples of the probability distribution of interest. <br /> Recent work showed that such generative models can be utilized in statistical mechanics to sample equilibrium states of many-body systems in physics and chemistry.  To scale and generalize these results, it is essential that the natural symmetries in the probability density – in physics defined by the invariances of the target potential – are built into the flow. <br /> We provide a theoretical sufficient criterium showing that the distribution generated by \textit{equivariant} normalizing flows is invariant with respect to these symmetries by design.  Furthermore, we propose building blocks for flows preserving symmetries which are usually found in physical/chemical many-body particle systems. Using benchmark systems motivated from molecular physics, we demonstrate that those symmetry preserving flows can provide better generalization capabilities and sampling efficiency.</p> 
### 1067.[PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination](https://proceedings.icml.cc/book/4308.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6722-Paper.pdf)
  Saurabh Goyal, Anamitra Roy Choudhury, Venkatesan Chakaravarthy, Saurabh Raje, Yogish Sabharwal, Ashish Verma [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6722-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6722-Supplemental.pdf)
> <p>We develop a novel method, called PoWER-BERT, for improving the inference time of the popular BERT model, while maintaining the accuracy. It works by: a) exploiting redundancy pertaining to word-vectors (intermediate encoder outputs) and eliminating the redundant vectors. b) determining which word-vectors to eliminate by developing a strategy for measuring their significance, based on the self-attention mechanism; c) learning how many word-vectors to eliminate by augmenting the BERT model and the loss function. Experiments on the standard GLUE benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference time over BERT with &lt; 1% loss in accuracy. We show that PoWER-BERT offers significantly better trade-off between accuracy and inference time compared to prior methods. We demonstrate that our method attains up to 6.8x reduction in inference time with &lt; 1% loss in accuracy when applied over ALBERT, a highly compressed version of BERT.</p> 
### 1068.[Bayesian Sparsification of Deep C-valued Networks](https://proceedings.icml.cc/book/4309.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6728-Paper.pdf)
  Ivan Nazarov, Evgeny Burnaev [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6728-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6728-Supplemental.zip)
> <p>With continual miniaturization ever more applications of deep learning can be found in embedded systems, where it is common to encounter data with natural representation in the complex domain. To this end we extend Sparse Variational Dropout to complex-valued neural networks and verify the proposed Bayesian technique by conducting a large numerical study of the performance-compression trade-off of C-valued networks on two tasks: image recognition on MNIST-like and CIFAR10 datasets and music transcription on MusicNet. We replicate the state-of-the-art result by Trabelsi et al. (2018) on MusicNet with a complex-valued network compressed by 50-100x at a small performance penalty.</p> 
### 1069.[Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack](https://proceedings.icml.cc/book/4310.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6735-Paper.pdf)
  Francesco Croce, Matthias Hein [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6735-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6735-Supplemental.pdf)
> The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one $l_p$-norm, and is robust to the phenomenon of gradient obfuscation.
### 1070.[A distributional view on multi objective policy optimization](https://proceedings.icml.cc/book/4311.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6749-Paper.pdf)
  Abbas Abdolmaleki, Sandy Huang, Leonard Hasenclever, Michael Neunert, Martina Zambelli, Murilo Martins, Francis Song, Nicolas Heess, Raia Hadsell, Martin Riedmiller [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6749-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6749-Supplemental.pdf)
> <p>Many real-world problems require trading off multiple competing objectives. However, these objectives are often in different units with different scales, which can make it challenging for practitioners to express numerical preferences over objectives in their native units. In this paper we propose a novel algorithm for multi-objective reinforcement learning that enables setting desired preferences for objectives in a scale-invariant way. We propose to learn a target local policy for each objective, and we use supervised learning to fit a parametric policy to a combination of these distributions. We demonstrate the effectiveness of our approach on challenging high-dimensional real and simulated robotics tasks, and show that setting different preferences in our framework allows us to trace out the space of nondominated solutions.</p> 
### 1071.[On the Sample Complexity of Adversarial Multi-Source PAC Learning](https://proceedings.icml.cc/book/4312.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6805-Paper.pdf)
  Nikola Konstantinov, Elias Frantar, Dan Alistarh, Christoph H. Lampert [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6805-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6805-Supplemental.pdf)
> <p>We study the problem of learning from multiple untrusted data sources,  a scenario of increasing practical relevance given the recent emergence of crowdsourcing and collaborative learning paradigms. Specifically, we analyze the situation in which a learning system obtains datasets from multiple sources, some of which might be biased or even adversarially perturbed. It is known that in the single-source case, an adversary with the power to corrupt a fixed fraction of the training data can prevent PAC-learnability, that is, even in the limit of infinitely much training data, no learning system can approach the optimal test error. In this work we show that, surprisingly, the same is not true in the multi-source setting, where the adversary can arbitrarily corrupt a fixed fraction of the data sources. Our main results are a generalization bound that provides finite-sample guarantees for this learning setting, as well as corresponding lower bounds. Besides establishing PAC-learnability our results also show that in a cooperative learning setting sharing data with other parties has provable benefits, even if some participants are malicious. </p> 
### 1072.[Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks](https://proceedings.icml.cc/book/4313.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6820-Paper.pdf)
  Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson, Sage Moore, Nir Shavit, Dan Alistarh [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6820-Metadata.json)
> <p>Optimizing convolutional neural networks for fast inference has recently become an extremely active area of research. One of the go-to solutions in this context is weight pruning, which aims to reduce computational and memory footprint by removing large subsets of the connections in a neural network. Surprisingly, much less attention has been given to exploiting sparsity in the activation maps, which tend to be naturally sparse in many settings thanks to the structure of rectified linear (ReLU) activation functions.</p>  <p>In this paper, we present an in-depth analysis of methods for maximizing the sparsity of the activations in a trained neural network, and show that, when coupled with an efficient sparse-input convolution algorithm, we can leverage this sparsity for significant performance gains. To induce highly sparse activation maps without accuracy loss, we introduce a new regularization technique, coupled with a new threshold-based sparsification method based on a parameterized activation function called Forced-Activation-Threshold Rectified Linear Unit (FATReLU). We examine the impact of our methods on popular image classification models, showing that most architectures can adapt to significantly sparser activation maps without any accuracy loss. Our second contribution is showing that these these compression gains can be translated into inference speedups: we provide a new algorithm to enable fast convolution operations over networks with sparse activations, and show that it can enable significant speedups for end-to-end inference on a range of popular models on the large-scale ImageNet image classification task on modern Intel CPUs, with little or no retraining cost.</p> 
### 1073.[Constructive universal distribution generation through deep ReLU networks](https://proceedings.icml.cc/book/4314.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6831-Paper.pdf)
  Dmytro Perekrestenko, Stephan Müller, Helmut Bölcskei [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6831-Metadata.json)
> <p>We present an explicit deep network construction that transforms uniformly distributed one-dimensional noise into an arbitrarily close approximation of any two-dimensional target distribution of finite differential entropy and Lipschitz-continuous pdf. The key ingredient of our design is a generalization of the  "space-filling'' property of sawtooth functions introduced in (Bailey &amp; Telgarsky, 2018). We elicit the importance of depth  in our construction in driving the Wasserstein distance between the target distribution and its approximation realized by the proposed neural network to zero. Finally, we outline how our construction can be extended to output distributions of arbitrary dimension.</p> 
### 1074.[Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks](https://proceedings.icml.cc/book/4315.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6846-Paper.pdf)
  Francesco Croce, Matthias Hein [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6846-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6846-Supplemental.pdf)
> The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and  user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 40 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than $10\%$, identifying several broken defenses.
### 1075.[Multiclass Neural Network Minimization via Tropical Newton Polytope Approximation](https://proceedings.icml.cc/book/4316.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6849-Paper.pdf)
  Georgios Smyrnis, Petros Maragos [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6849-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6849-Supplemental.zip)
> <p>The field of tropical algebra is closely linked with the domain of neural networks with piecewise linear activations, since their output can be described via tropical polynomials in the max-plus semiring.  In this work, we attempt to make use of methods stemming from a form of approximate division of such polynomials, which relies on the approximation of their Newton Polytopes, in order to minimize networks trained for multiclass classification problems. We make theoretical contributions in this domain, by proposing and analyzing methods which seek to reduce the size of such networks. In addition, we make experimental evaluations on the MNIST and Fashion-MNIST datasets, with our results demonstrating a significant reduction in network size, while retaining adequate performance.</p> 
### 1076.[Finding trainable sparse networks through Neural Tangent Transfer ](https://proceedings.icml.cc/book/4317.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6851-Paper.pdf)
  Tianlin Liu, Friedemann Zenke [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6851-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6851-Supplemental.pdf)
> <p>Deep neural networks have dramatically transformed machine learning, but their memory and energy demands are substantial. The requirements of real biological neural networks are rather modest in comparison, and one feature that might underlie this austerity is their sparse connectivity. In deep learning, trainable sparse networks that perform well on a specific task are usually constructed using label-dependent pruning criteria.  In this article, we introduce Neural Tangent Transfer, a method that instead finds trainable sparse networks in a label-free manner. Specifically, we find sparse networks whose training dynamics, as characterized by the neural tangent kernel, mimic those of dense networks in function space. Finally, we evaluate our label-agnostic approach on several standard classification tasks and show that the resulting sparse networks achieve higher classification performance while converging faster.</p> 
### 1077.[Towards a General Theory of Infinite-Width Limits of Neural Classifiers](https://proceedings.icml.cc/book/4318.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6879-Paper.pdf)
  Eugene Golikov [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6879-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6879-Supplemental.pdf)
> <p>Generally, obtaining theoretical guarantees for neural networks training appears to be a hard problem. Recent research has been focused on studying this problem in the limit of infinite width and two different theories have been developed: mean-field (MF) limit theory and kernel limit theory. We propose a general framework that provides a link between these seemingly distinct limit theories. Our framework out of the box gives rise to a discrete-time MF limit — a setup that to the best of our knowledge was not previously explored in literature.  We prove a convergence theorem for it and show that it provides a more reasonable approximation for finite-width nets compared to NTK limit if learning rates are not very small. Also, our framework suggests a different type of infinite-width limits, not covered by both MF and kernel limit theories. We show that for networks with more than two hidden layers RMSProp training has a non-trivial MF limit, but GD training does not have one. Overall, our framework demonstrates that both MF and NTK limits have considerable limitations in approximating finite-sized neural nets, indicating the need for designing more accurate infinite-width approximations for them.</p> 
### 1078.[Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics](https://proceedings.icml.cc/book/4319.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6884-Paper.pdf)
  Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, Dmitry Vetrov [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6884-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6884-Supplemental.pdf)
> <p>According to previous studies, one of the major impediments to accurate off-policy learning is the overestimation bias. This paper investigates a novel way to alleviate the overestimation bias in a continuous control setting. Our method---Truncated Quantile Critics, TQC,---blends three ideas: distributional representation of a critic, truncation of critics prediction, and ensembling of multiple critics. We show that all components are key for the achieved performance. Distributional representation combined with truncation allows for arbitrary granular overestimation control, and ensembling further improves the results of our method. TQC significantly outperforms the current state of the art on all environments from the continuous control benchmark suite, demonstrating 25% improvement on the most challenging Humanoid environment.</p> 
### 1079.[Learning to Learn Kernels with Variational Random Features](https://proceedings.icml.cc/book/4320.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6885-Paper.pdf)
  Xiantong Zhen, Haoliang Sun, Yingjun Du, Jun Xu, Yilong Yin, Ling Shao, Cees Snoek [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6885-Metadata.json)
> <p>We introduce kernels with random Fourier features in the meta-learning framework for few-shot learning. We propose meta variational random features (MetaVRF) to learn adaptive kernels for the base-learner, which is developed in a latent variable model by treating the random feature basis as the latent variable. We formulate the optimization of MetaVRF as a variational inference problem by deriving an evidence lower bound under the meta-learning framework. To incorporate shared knowledge from related tasks, we propose a context inference of the posterior, which is established by an LSTM architecture. The LSTM-based inference network can effectively integrate the context information of previous tasks with task-specific information, generating informative and adaptive features. The learned MetaVRF can produce kernels of high representational power with a relatively low spectral sampling rate and also enables fast adaptation to new tasks. Experimental results on a variety of few-shot regression and classification tasks demonstrate that MetaVRF delivers much better, or at least competitive, performance compared to existing meta-learning alternatives.</p> 
### 1080.[Efficient Robustness Certificates for Discrete Data: Sparsity-Aware Randomized Smoothing for Graphs, Images and More](https://proceedings.icml.cc/book/4321.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6890-Paper.pdf)
  Aleksandar Bojchevski, Johannes Klicpera, Stephan Günnemann [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6890-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6890-Supplemental.pdf)
> <p>Existing techniques for certifying robustness of models for discrete data either work only for a small class of models or are general at the expense of efficiency or tightness. Moreover, they do not account for sparsity in the input which, as our findings show, is often essential for obtaining non-trivial guarantees. We propose a model-agnostic certificate based on the randomized smoothing framework which subsumes earlier work and is tight, efficient, and sparsity-aware. Its computational complexity does not depend on the number of discrete categories or the dimension of the input (e.g. the graph size), making it highly scalable. We show the effectiveness of our approach on a wide variety of models, datasets, and tasks -- specifically highlighting its use for Graph Neural Networks. GNNs have become widely used, yet are highly sensitive to adversarial attacks. So far, obtaining provable guarantees has been difficult due to the discrete and non-i.i.d. nature of graph data. Our method can certify any GNN and handles perturbations to both the graph structure and the node attributes.</p> 
### 1081.[Learning to Simulate Complex Physics with Graph Networks](https://proceedings.icml.cc/book/4322.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6892-Paper.pdf)
  Alvaro Sanchez, Jonathan Godwin, Tobias Pfaff, Rex (Zhitao) Ying, Jure Leskovec, Peter Battaglia [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6892-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6892-Supplemental.pdf)
> <p>Here we present a general framework for learning simulation, and provide a single model implementation that yields state-of-the-art performance across a variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term "Graph Network-based Simulators"" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework is the most accurate general-purpose learned physics simulator to date, and holds promise for solving a wide range of complex forward and inverse problems.</p> 
### 1082.[Small Data, Big Decisions: Model Selection in the Small-Data Regime](https://proceedings.icml.cc/book/4323.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6899-Paper.pdf)
  Jorg Bornschein, Francesco Visin, Simon Osindero [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6899-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6899-Supplemental.pdf)
> <p>Highly overparametrized neural networks can display curiously strong generalization performance -- a phenomenon that has recently garnered a wealth of theoretical and empirical research in order to better understand it. In contrast to most previous work, which typically considers the performance as a function of the model size, in this paper we empirically study the generalization performance as the size of the training set varies over multiple orders of magnitude. These systematic experiments lead to some interesting and potentially very useful  observations; perhaps most notably that training on smaller subsets of the data can lead to more reliable model selection decisions whilst simultaneously enjoying smaller computational overheads. Our experiments furthermore allow us to estimate Minimum Description Lengths for common datasets given modern neural network architectures, thereby paving the way for principled model selection taking into account Occams-razor.</p> 
### 1083.[PolyGen: An Autoregressive Generative Model of 3D Meshes](https://proceedings.icml.cc/book/4324.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6917-Paper.pdf)
  Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, Peter Battaglia [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6917-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6917-Supplemental.pdf)
> <p>Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present an approach which models the mesh directly, predicting mesh vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is fully probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task.  We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task. </p> 
### 1084.[XtarNet: Learning to Extract Task-Adaptive Representation for Incremental Few-Shot Learning](https://proceedings.icml.cc/book/4325.pdf)[ :link: ](https://proceedings.icml.cc/static/paper_files/icml/2020/6928-Paper.pdf)
  Sung Whan Yoon, Jun Seo, Doyeon Kim, Jaekyun Moon [Metadata](https://proceedings.icml.cc/static/paper_files/icml/2020/6928-Metadata.json) [Supplemental](https://proceedings.icml.cc/static/paper_files/icml/2020/6928-Supplemental.pdf)
> <p>Learning novel concepts while preserving prior knowledge is a long-standing challenge in machine learning. The challenge gets greater when a novel task is given with only a few labeled examples, a problem known as incremental few-shot learning. We propose XtarNet, which learns to extract task-adaptive representation (TAR) for facilitating incremental few-shot learning. The method utilizes a backbone network pretrained on a set of base categories while also employing additional modules that are meta-trained across episodes. Given a new task, the novel feature extracted from the meta-trained modules is mixed with the base feature obtained from the pretrained model. The process of combining two different features provides TAR and is also controlled by meta-trained modules. The TAR contains effective information for classifying both novel and base categories. The base and novel classifiers quickly adapt to a given task by utilizing the TAR. Experiments on standard image datasets indicate that XtarNet achieves state-of-the-art incremental few-shot learning performance. The concept of TAR can also be used in conjunction with existing incremental few-shot learning methods; extensive simulation results in fact show that applying TAR enhances the known methods significantly.</p> 

